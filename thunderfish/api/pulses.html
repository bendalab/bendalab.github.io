<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>thunderfish.pulses API documentation</title>
<meta name="description" content="Extract and cluster EOD waverforms of pulse-type electric fish â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>thunderfish.pulses</code></h1>
</header>
<section id="section-intro">
<p>Extract and cluster EOD waverforms of pulse-type electric fish.</p>
<h2 id="main-function">Main function</h2>
<ul>
<li><code><a title="thunderfish.pulses.extract_pulsefish" href="#thunderfish.pulses.extract_pulsefish">extract_pulsefish()</a></code>: checks for pulse-type fish based on the EOD amplitude and shape.</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Extract and cluster EOD waverforms of pulse-type electric fish.

## Main function

- `extract_pulsefish()`: checks for pulse-type fish based on the EOD amplitude and shape.

&#34;&#34;&#34;

import os
import numpy as np
from scipy import stats
from scipy.interpolate import interp1d
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.mixture import BayesianGaussianMixture
from sklearn.metrics import pairwise_distances
from .eventdetection import detect_peaks, median_std_threshold
from .pulseplots import *

import warnings
def warn(*args, **kwargs):
    &#34;&#34;&#34;
    Ignore all warnings.
    &#34;&#34;&#34;
    pass
warnings.warn = warn

try:
    from numba import jit
except ImportError:
    def jit(*args, **kwargs):
        def decorator_jit(func):
            return func
        return decorator_jit


# upgrade numpy functions for backwards compatibility:
if not hasattr(np, &#39;isin&#39;):
    np.isin = np.in1d

def unique_counts(ar):
    &#34;&#34;&#34; Find the unique elements of an array and their counts, ignoring shape.

    The code is condensed from numpy version 1.17.0.
    
    Parameters
    ----------
    ar : numpy array
        Input array

    Returns
    -------
    unique_vaulues : numpy array
        Unique values in array ar.
    unique_counts : numpy array
        Number of instances for each unique value in ar.
    &#34;&#34;&#34;
    try:
        return np.unique(ar, return_counts=True)
    except TypeError:
        ar = np.asanyarray(ar).flatten()
        ar.sort()
        mask = np.empty(ar.shape, dtype=np.bool_)
        mask[:1] = True
        mask[1:] = ar[1:] != ar[:-1]
        idx = np.concatenate(np.nonzero(mask) + ([mask.size],))
        return ar[mask], np.diff(idx)  


###################################################################################


def extract_pulsefish(data, samplerate, width_factor_shape=3, width_factor_wave=8,
                      width_factor_display=4, verbose=0, plot_level=0, save_plots=False,
                      save_path=&#39;&#39;, ftype=&#39;png&#39;, return_data=[]):
    &#34;&#34;&#34; Extract and cluster pulse-type fish EODs from single channel data.
    
    Takes recording data containing an unknown number of pulsefish and extracts the mean 
    EOD and EOD timepoints for each fish present in the recording.
    
    Parameters
    ----------
    data: 1-D array of float
        The data to be analysed.
    samplerate: float
        Sampling rate of the data in Hertz.

    width_factor_shape : float (optional)
        Width multiplier used for EOD shape analysis.
        EOD snippets are extracted based on width between the 
        peak and trough multiplied by the width factor.
    width_factor_wave : float (optional)
        Width multiplier used for wavefish detection.
    width_factor_display : float (optional)
        Width multiplier used for EOD mean extraction and display.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plots : bool (optional)
        Set to True to save the plots created by plot_level.
    save_path: string (optional)
        Path for saving plots.
    ftype : string (optional)
        Define the filetype to save the plots in if save_plots is set to True.
        Options are: &#39;png&#39;, &#39;jpg&#39;, &#39;svg&#39; ...
    return_data : list of strings (optional)
        Specify data that should be logged and returned in a dictionary. Each clustering 
        step has a specific keyword that results in adding different variables to the log dictionary.
        Optional keys for return_data and the resulting additional key-value pairs to the log dictionary are:
        
        - &#39;all_eod_times&#39;:
            - &#39;all_times&#39;:  list of two lists of floats.
                All peak (`all_times[0]`) and trough times (`all_times[1]`) extracted
                by the peak detection algorithm. Times are given in seconds.
            - &#39;eod_troughtimes&#39;: list of 1D arrays.
                The timepoints in seconds of each unique extracted EOD cluster,
                where each 1D array encodes one cluster.
        
        - &#39;peak_detection&#39;:
            - &#34;data&#34;: 1D numpy array of floats.
                Quadratically interpolated data which was used for peak detection.
            - &#34;interp_fac&#34;: float.
                Interpolation factor of raw data.
            - &#34;peaks_1&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after first peak detection step.
            - &#34;troughs_1&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after first peak detection step.
            - &#34;peaks_2&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after second peak detection step.
            - &#34;troughs_2&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after second peak detection step.
            - &#34;peaks_3&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after third peak detection step.
            - &#34;troughs_3&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after third peak detection step.
            - &#34;peaks_4&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after fourth peak detection step.
            - &#34;troughs_4&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after fourth peak detection step.

        - &#39;all_cluster_steps&#39;:
            - &#39;samplerate&#39;: float.
                Samplerate of interpolated data.
            - &#39;EOD_widths&#39;: list of three 1D numpy arrays.
                The first list entry gives the unique labels of all width clusters
                as a list of ints.
                The second list entry gives the width values for each EOD in samples
                as a 1D numpy array of ints.
                The third list entry gives the width labels for each EOD
                as a 1D numpy array of ints.
            - &#39;EOD_heights&#39;: nested lists (2 layers) of three 1D numpy arrays.
                The first list entry gives the unique labels of all height clusters
                as a list of ints for each width cluster.
                The second list entry gives the height values for each EOD
                as a 1D numpy array of floats for each width cluster.
                The third list entry gives the height labels for each EOD
                as a 1D numpy array of ints for each width cluster.
            - &#39;EOD_shapes&#39;: nested lists (3 layers) of three 1D numpy arrays
                The first list entry gives the raw EOD snippets as a 2D numpy array
                for each height cluster in a width cluster.
                The second list entry gives the snippet PCA values for each EOD
                as a 2D numpy array of floats for each height cluster in a width cluster.
                The third list entry gives the shape labels for each EOD as a 1D numpy array
                of ints for each height cluster in a width cluster.
            - &#39;discarding_masks&#39;: Nested lists (two layers) of 1D numpy arrays.
                The masks of EODs that are discarded by the discarding step of the algorithm.
                The masks are 1D boolean arrays where instances that are set to True are
                discarded by the algorithm. Discarding masks are saved in nested lists
                that represent the width and height clusters.
            - &#39;merge_masks&#39;: Nested lists (two layers) of 2D numpy arrays.
                The masks of EODs that are discarded by the merging step of the algorithm.
                The masks are 2D boolean arrays where for each sample point `i` either
                `merge_mask[i,0]` or `merge_mask[i,1]` is set to True. Here, merge_mask[:,0]
                represents the peak-centered clusters and `merge_mask[:,1]` represents the
                trough-centered clusters. Merge masks are saved in nested lists that
                represent the width and height clusters.

        - &#39;BGM_width&#39;:
            - &#39;BGM_width&#39;: dictionary
                - &#39;x&#39;: 1D numpy array of floats.
                    BGM input values (in this case the EOD widths),
                - &#39;use_log&#39;: boolean.
                    True if the z-scored logarithm of the data was used as BGM input.
                - &#39;BGM&#39;: list of three 1D numpy arrays.
                    The first instance are the weights of the Gaussian fits.
                    The second instance are the means of the Gaussian fits.
                    The third instance are the variances of the Gaussian fits.
                - &#39;labels&#39;: 1D numpy array of ints.
                    Labels defined by BGM model (before merging based on merge factor).
                - xlab&#39;: string.
                    Label for plot (defines the units of the BGM data).

        - &#39;BGM_height&#39;:
            This key adds a new dictionary for each width cluster.
            - &#39;BGM_height_*n*&#39; : dictionary, where *n* defines the width cluster as an int.
                - &#39;x&#39;: 1D numpy array of floats.
                    BGM input values (in this case the EOD heights),
                - &#39;use_log&#39;: boolean.
                    True if the z-scored logarithm of the data was used as BGM input.
                - &#39;BGM&#39;: list of three 1D numpy arrays.
                    The first instance are the weights of the Gaussian fits.
                    The second instance are the means of the Gaussian fits.
                    The third instance are the variances of the Gaussian fits.
                - &#39;labels&#39;: 1D numpy array of ints.
                    Labels defined by BGM model (before merging based on merge factor).
                - &#39;xlab&#39;: string.
                    Label for plot (defines the units of the BGM data).

        - &#39;snippet_clusters&#39;:
            This key adds a new dictionary for each height cluster.
            - &#39;snippet_clusters*_n_m_p*&#39; : dictionary, where *n* defines the width cluster
              (int), *m* defines the height cluster (int) and *p* defines shape clustering
              on peak or trough centered EOD snippets (string: &#39;peak&#39; or &#39;trough&#39;).
                - &#39;raw_snippets&#39;: 2D numpy array (nsamples, nfeatures).
                    Raw EOD snippets.
                - &#39;snippets&#39;: 2D numpy array.
                    Normalized EOD snippets.
                - &#39;features&#39;: 2D numpy array.(nsamples, nfeatures)
                    PCA values for each normalized EOD snippet.
                - &#39;clusters&#39;: 1D numpy array of ints.
                    Cluster labels.
                - &#39;samplerate&#39;: float.
                    Samplerate of snippets.

        - &#39;eod_deletion&#39;:
            This key adds two dictionaries for each (peak centered) shape cluster,
            where *cluster* (int) is the unique shape cluster label.
            - &#39;mask_*cluster*&#39; : list of four booleans.
                The mask for each cluster discarding step. 
                The first instance represents the artefact masks, where artefacts
                are set to True.
                The second instance represents the unreliable cluster masks,
                where unreliable clusters are set to True.
                The third instance represents the wavefish masks, where wavefish
                are set to True.
                The fourth instance represents the sidepeak masks, where sidepeaks
                are set to True.
            - &#39;vals_*cluster*&#39; : list of lists.
                All variables that are used for each cluster deletion step.
                The first instance is a list of two 1D numpy arrays: the mean EOD and
                the FFT of that mean EOD.
                The second instance is a 1D numpy array with all EOD width to ISI ratios.
                The third instance is a list with three entries: 
                    The first entry is a 1D numpy array zoomed out version of the mean EOD.
                    The second entry is a list of two 1D numpy arrays that define the peak
                    and trough indices of the zoomed out mean EOD.
                    The third entry contains a list of two values that represent the
                    peak-trough pair in the zoomed out mean EOD with the largest height
                    difference.
            - &#39;samplerate&#39; : float.
                EOD snippet samplerate.

        - &#39;masks&#39;: 
            - &#39;masks&#39; : 2D numpy array (4,N).
                Each row contains masks for each EOD detected by the EOD peakdetection step. 
                The first row defines the artefact masks, the second row defines the
                unreliable EOD masks, 
                the third row defines the wavefish masks and the fourth row defines
                the sidepeak masks.

        - &#39;moving_fish&#39;:
            - &#39;moving_fish&#39;: dictionary.
                - &#39;w&#39; : list of floats.
                    Median width for each width cluster that the moving fish algorithm is
                    computed on (in seconds).
                - &#39;T&#39; : list of floats.
                    Lenght of analyzed recording for each width cluster (in seconds).
                - &#39;dt&#39; : list of floats.
                    Sliding window size (in seconds) for each width cluster.
                - &#39;clusters&#39; : list of 1D numpy int arrays.
                    Cluster labels for each EOD cluster in a width cluster.
                - &#39;t&#39; : list of 1D numpy float arrays.
                    EOD emission times for each EOD in a width cluster.
                - &#39;fishcount&#39; : list of lists.
                    Sliding window timepoints and fishcounts for each width cluster.
                - &#39;ignore_steps&#39; : list of 1D int arrays.
                    Mask for fishcounts that were ignored (ignored if True) in the
                    moving_fish analysis.
        
    Returns
    -------
    mean_eods: list of 2D arrays (3, eod_length)
        The average EOD for each detected fish. First column is time in seconds,
        second column the mean eod, third column the standard error.
    eod_times: list of 1D arrays
        For each detected fish the times of EOD peaks or troughs in seconds.
        Use these timepoints for EOD averaging.
    eod_peaktimes: list of 1D arrays
        For each detected fish the times of EOD peaks in seconds.
    zoom_window: tuple of floats
        Start and endtime of suggested window for plotting EOD timepoints.
    log_dict: dictionary
        Dictionary with logged variables, where variables to log are specified
        by `return_data`.
    &#34;&#34;&#34;
    if verbose &gt; 0:
        print(&#39;&#39;)
        if verbose &gt; 1:
            print(70*&#39;#&#39;)
        print(&#39;##### extract_pulsefish&#39;, 46*&#39;#&#39;)

    if (save_plots and plot_level&gt;0 and save_path):
        # create folder to save things in.
        if not os.path.exists(save_path):
            os.makedirs(save_path)
    else:
        save_path = &#39;&#39;

    mean_eods, eod_times, eod_peaktimes, zoom_window = [], [], [], []
    log_dict = {}

    # interpolate:
    i_samplerate = 500000.0
    #i_samplerate = samplerate
    try:
        f = interp1d(np.arange(len(data))/samplerate, data, kind=&#39;quadratic&#39;)
        i_data = f(np.arange(0.0, (len(data)-1)/samplerate, 1.0/i_samplerate))
    except MemoryError:
        i_samplerate = samplerate
        i_data = data
    log_dict[&#39;data&#39;] = i_data               # TODO: could be removed
    log_dict[&#39;samplerate&#39;] = i_samplerate   # TODO: could be removed
    log_dict[&#39;i_data&#39;] = i_data
    log_dict[&#39;i_samplerate&#39;] = i_samplerate
    # log_dict[&#34;interp_fac&#34;] = interp_fac  # TODO: is not set anymore
                                         
    
    # extract peaks:
    x_peak, x_trough, eod_heights, eod_widths, pd_log_dict = \
      detect_pulse_candidates(i_data, i_samplerate,
                              np.max([width_factor_shape, width_factor_display, width_factor_wave]),
                              verbose=verbose-1, return_data=return_data)
    
    if len(x_peak) &gt; 0:
        # cluster
        clusters, x_merge, c_log_dict = cluster(x_peak, x_trough, eod_heights, eod_widths,
                                                i_data, i_samplerate,
                                                width_factor_shape, width_factor_wave,
                                                verbose=verbose-1, plot_level=plot_level-1,
                                                save_plots=save_plots, save_path=save_path,
                                                ftype=ftype, return_data=return_data) 

        # extract mean eods and times
        mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels = \
          extract_means(i_data, x_merge, x_peak, x_trough, eod_widths, clusters,
                        i_samplerate, width_factor_display, verbose=verbose-1)

        # determine clipped clusters (save them, but ignore in other steps)
        clusters, clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes = \
          find_clipped_clusters(clusters, mean_eods, eod_times, eod_peaktimes,
                                eod_troughtimes, cluster_labels, width_factor_display,
                                verbose=verbose-1)

        # delete the moving fish
        clusters, zoom_window, mf_log_dict = \
          delete_moving_fish(clusters, x_merge/i_samplerate, len(data)/samplerate,
                             eod_heights, eod_widths/i_samplerate, i_samplerate,
                             verbose=verbose-1, plot_level=plot_level-1, save_plot=save_plots,
                             save_path=save_path, ftype=ftype, return_data=return_data)
        
        if &#39;moving_fish&#39; in return_data:
            log_dict[&#39;moving_fish&#39;] = mf_log_dict

        clusters = remove_sparse_detections(clusters, eod_widths, i_samplerate,
                                            len(data)/samplerate, verbose=verbose-1)

        # extract mean eods
        mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels = \
          extract_means(i_data, x_merge, x_peak, x_trough, eod_widths,
                        clusters, i_samplerate, width_factor_display, verbose=verbose-1)

        mean_eods.extend(clipped_eods)
        eod_times.extend(clipped_times)
        eod_peaktimes.extend(clipped_peaktimes)
        eod_troughtimes.extend(clipped_troughtimes)

        if plot_level &gt; 0:
            plot_all(data, eod_peaktimes, eod_troughtimes, samplerate, mean_eods)
            if save_plots:
                plt.savefig(&#39;%sextract_pulsefish_results.%s&#39; % (save_path, ftype))
        if save_plots:
            plt.close(&#39;all&#39;)
    
        if &#39;all_eod_times&#39; in return_data:
            log_dict[&#39;all_times&#39;] = [x_peak/i_samplerate, x_trough/i_samplerate]
            log_dict[&#39;eod_troughtimes&#39;] = eod_troughtimes
        
        log_dict.update(pd_log_dict)
        log_dict.update(c_log_dict)

    return mean_eods, eod_times, eod_peaktimes, zoom_window, log_dict


def detect_pulse_candidates(data, samplerate, width_factor,
                            min_peakwidth=0.00005, max_peakwidth=0.01,
                            verbose=0, return_data=[]):
    &#34;&#34;&#34; Detect potential pulse EODs in data.

    Was `def extract_eod_times(data, samplerate, width_factor, interp_freq=500000,
                      max_peakwidth=0.01, min_peakwidth=None, verbose=0,
                      return_data=[], save_path=&#39;&#39;)` before.

    Parameters
    ----------
    data: 1-D array of float
        The data to be analysed.
    samplerate: float
        Sampling rate of the data
    width_factor: float
        Factor for extracting EOD shapes.
        Only EODs are extracted that can fully be analysed with this width.

    min_peakwidth: float (optional)
        Minimum width for peak detection in seconds.
    max_peakwidth: float (optional)
        Maximum width for peak detection in seconds.

    verbose : int (optional)
        Verbosity level.
    return_data : list of strings (optional)
        Keys that specify data to be logged. If &#39;peak_detection&#39; is in `return_data`, 
        data of this function is logged (see extract_pulsefish()).

    Returns
    -------
    x_peak: array of ints
        Indices of EOD peaks in data.
    x_trough: array of ints
        Indices of EOD troughs in data. There is one x_trough for each x_peak.
    eod_heights: array of floats
        EOD heights for each x_peak.
    eod_widths: array of ints
        EOD widths for each x_peak (in samples).
    peak_detection_result : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.

    &#34;&#34;&#34;
    peak_detection_result = {}
    
    # standard deviation of data in small snippets:
    threshold = median_std_threshold(data, samplerate)  # TODO make this a parameter

    # detect peaks and troughs in the data:
    peak_indices, trough_indices = detect_peaks(data, threshold)
    if verbose &gt; 0:
        print(&#39;Peaks/troughs detected in data:                      %5d %5d&#39;
              % (len(peak_indices), len(trough_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_1=np.array(peak_indices),
                                     troughs_1=np.array(trough_indices))
    if len(peak_indices)&lt;2 or \
       len(trough_indices)&lt;2 or \
       len(peak_indices) &gt; len(data)/20:
        if verbose &gt; 0:
            print(&#39;No or too many peaks/troughs detected in data.&#39;)
        return np.array([]), np.array([]), np.array([]), np.array([]), \
            peak_detection_result

    # assign troughs to peaks:
    peak_indices, trough_indices, heights, widths = \
      assign_side_peaks(data, peak_indices, trough_indices, 0.25)
    # TODO: make 0.25 a parameter
    if verbose &gt; 1:
        print(&#39;Number of peaks after assigning side-peaks:          %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_2=np.array(peak_indices),
                                     troughs_2=np.array(trough_indices))

    # check widths:
    keep_events = ((widths&gt;min_peakwidth*samplerate) &amp;
                   (widths&lt;max_peakwidth*samplerate))
    peak_indices = peak_indices[keep_events]
    trough_indices = trough_indices[keep_events]
    heights = heights[keep_events]
    widths = widths[keep_events]
    if verbose &gt; 1:
        print(&#39;Number of peaks after checking pulse width:          %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_3=np.array(peak_indices),
                                     troughs_3=np.array(trough_indices))

    &#34;&#34;&#34;&#34;
    # merge connected peaks:
    peak_indices, trough_indices, heights, widths = \
      discard_connecting_eods(peak_indices, trough_indices, heights, widths)
    if verbose &gt; 1:
        print(&#39;Number of peaks after merging pulses:                %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_4=np.array(peak_indices),
                                     troughs_4=np.array(trough_indices))
    if len(peak_indices) == 0:
        if verbose &gt; 0:
            print(&#39;No peaks remain as pulse candidates.&#39;)
        return np.array([]), np.array([]), np.array([]), np.array([]), \
            peak_detection_result
    &#34;&#34;&#34;
    
    # only take those where the maximum cutwidth does not cause issues -
    # if the width_factor times the width + x is more than length.
    max_width = np.max(widths)*width_factor
    cut_idx = ((peak_indices - max_width &gt; 0) &amp;
               (peak_indices + max_width &lt; len(data)) &amp;
               (trough_indices - max_width &gt; 0) &amp;
               (trough_indices + max_width &lt; len(data)))

    if verbose &gt; 0:
        print(&#39;Remaining peaks after EOD extraction:                %5d&#39;
              % (p.sum(cut_idx)))
        print(&#39;&#39;)

    return peak_indices[cut_idx], trough_indices[cut_idx], heights[cut_idx], widths[cut_idx], peak_detection_result


def assign_side_peaks(data, peak_indices, trough_indices,
                      min_rel_slope_diff=0.25):
    &#34;&#34;&#34;Assign to each peak the trough resulting in a pulse with the steepest slope or largest height.

    The slope between a peak and a trough is computed as the height
    difference divided by the distance between peak and trough. If the
    slopes between the left and the right trough differ by less than
    `min_rel_slope_diff`, then just the heigths between and the two
    troughs relative to the peak are compared.

    Was `def detect_eod_peaks(data, main_indices, side_indices,
                              max_width=20, min_width=2, verbose=0)` before.

    Parameters
    ----------
    data: array of floats
        Data in which the events were detected.
    peak_indices: array of ints
        Indices of the detected peaks in the data time series.
    trough_indices: array of ints
        Indices of the detected troughs in the data time series. 
    min_rel_slope_diff: float
        Minimum required difference of left and right slope relative
        to mean slope.

    Returns
    -------
    peak_indices: array of ints
        Peak indices. Same as input `peak_indices` but potentially shorter
        by one or two elements.
    trough_indices: array of ints
        Corresponding trough indices of trough to the left or right
        of the peaks.
    heights: array of floats
        Peak heights (distance between peak and corresponding trough amplitude)
    widths: array of ints
        Peak widths (distance between peak and corresponding trough indices)

    &#34;&#34;&#34;
    # is a main or side peak first?
    peak_first = int(peak_indices[0] &lt; trough_indices[0])
    # is a main or side peak last?
    peak_last = int(peak_indices[-1] &gt; trough_indices[-1])
    # ensure main peaks to have side peaks at both sides:
    peak_indices = peak_indices[peak_first:len(peak_indices)-peak_last]
    y = data[peak_indices]
    
    # indices of troughs on the left and right side of main peaks:
    l_indices = np.arange(len(peak_indices))
    r_indices = l_indices + 1

    # indices, distance to peak, height, and slope of left troughs:
    l_side_indices = trough_indices[l_indices]
    l_distance = np.abs(peak_indices - l_side_indices)
    l_height = np.abs(y - data[l_side_indices])
    l_slope = np.abs(l_height/l_distance)

    # indices, distance to peak, height, and slope of right troughs:
    r_side_indices = trough_indices[r_indices]
    r_distance = np.abs(r_side_indices - peak_indices)
    r_height = np.abs(y - data[r_side_indices])
    r_slope = np.abs(r_height/r_distance)

    # which trough to assign to the peak?
    # - either the one with the steepest slope, or
    # - when slopes are similar on both sides (within 25% difference),
    #   the trough with the maximum height difference to the peak:
    rel_slopes = np.abs(l_slope-r_slope)/(0.5*(l_slope+r_slope))
    take_slopes = rel_slopes &gt; min_rel_slope_diff
    take_left = l_height &gt; r_height
    take_left[take_slopes] = l_slope[take_slopes] &gt; r_slope[take_slopes]

    # assign troughs, heights, and widths:
    trough_indices = np.where(take_left,
                              trough_indices[:-1], trough_indices[1:])
    heights = np.where(take_left, l_height, r_height)
    widths = np.where(take_left, l_distance, r_distance)
    slopes = np.where(take_left, l_slope, r_slope)

    ## TODO: check for width here?
    
    # discard connected peaks:
    same = np.nonzero(trough_indices[:-1] == trough_indices[1:])[0]
    keep = np.ones(len(trough_indices), dtype=np.bool)
    for i in same:
        # same troughs at trough_indices[i] and trough_indices[i+1]:
        s = slopes[i:i+2]
        rel_slopes = np.abs(np.diff(s))[0]/np.mean(s)
        if rel_slopes &gt; min_rel_slope_diff:
            keep[i+(s[1]&lt;s[0])] = False
        else:
            keep[i+(heights[i+1]&lt;heights[i])] = False
    
    return peak_indices[keep], trough_indices[keep], heights[keep], widths[keep]


@jit(nopython=True)
def discard_connecting_eods(x_peak, x_trough, heights, widths):
    &#34;&#34;&#34;If two detected EODs share the same closest trough, keep only the
highest peak.

    Parameters
    ----------
    x_peak: list of ints
        Indices of EOD peaks.
    x_trough: list of ints
        Indices of EOD troughs.
    heights: list of floats
        EOD heights.
    widths: list of ints
        EOD widths.

    Returns
    -------
    x_peak, x_trough, heights, widths : lists of ints and floats
        EOD location and features of the non-discarded EODs
    &#34;&#34;&#34;
    keep_idxs = np.ones(len(x_peak))

    for tr in np.unique(x_trough):
        if len(x_trough[x_trough==tr]) &gt; 1:
            slopes = heights[x_trough==tr]/widths[x_trough==tr]

            if (np.max(slopes)!=np.min(slopes)) and \
              (np.abs(np.max(slopes)-np.min(slopes))/(0.5*np.max(slopes)+0.5*np.min(slopes)) &gt; 0.25):
                keep_idxs[np.where(x_trough==tr)[0][np.argmin(heights[x_trough==tr]/widths[x_trough==tr])]] = 0
            else:
                keep_idxs[np.where(x_trough==tr)[0][np.argmin(heights[x_trough==tr])]] = 0
            
    return x_peak[np.where(keep_idxs==1)[0]], x_trough[np.where(keep_idxs==1)[0]], heights[np.where(keep_idxs==1)[0]], widths[np.where(keep_idxs==1)[0]]


def cluster(eod_xp, eod_xt, eod_heights, eod_widths, data, samplerate,
            width_factor_shape, width_factor_wave,
            n_gaus_height=10, merge_threshold_height=0.1, n_gaus_width=3,
            merge_threshold_width=0.5, minp=10,
            verbose=0, plot_level=0, save_plots=False, save_path=&#39;&#39;, ftype=&#39;pdf&#39;,
            return_data=[]):
    &#34;&#34;&#34;Cluster EODs.
    
    First cluster on EOD widths using a Bayesian Gaussian
    Mixture (BGM) model,  then cluster on EOD heights using a
    BGM model. Lastly, cluster on EOD waveform with DBSCAN.
    Clustering on EOD waveform is performed twice, once on
    peak-centered EODs and once on trough-centered EODs.
    Non-pulsetype EOD clusters are deleted, and clusters are
    merged afterwards.

    Parameters
    ----------
    eod_xp : list of ints
        Location of EOD peaks in indices.
    eod_xt: list of ints
        Locations of EOD troughs in indices.
    eod_heights: list of floats
        EOD heights.
    eod_widths: list of ints
        EOD widths in samples.
    data: array of floats
        Data in which to detect pulse EODs.
    samplerate : float
        Sample rate of `data`.
    width_factor_shape : float
        Multiplier for snippet extraction width. This factor is
        multiplied with the width between the peak and through of a
        single EOD.
    width_factor_wave : float
        Multiplier for wavefish extraction width.
    n_gaus_height : int (optional)
        Number of gaussians to use for the clustering based on EOD height.
    merge_threshold_height : float (optional)
        Threshold for merging clusters that are similar in height.
    n_gaus_width : int (optional)
        Number of gaussians to use for the clustering based on EOD width.
    merge_threshold_width : float (optional)
        Threshold for merging clusters that are similar in width.
    minp : int (optional)
        Minimum number of points for core clusters (DBSCAN).
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plots : bool (optional)
        Set to True to save created plots.
    save_path : string (optional)
        Path to save plots to. Only used if save_plots==True.
    ftype : string (optional)
        Filetype to save plot images in.
    return_data : list of strings (optional)
        Keys that specify data to be logged. Keys that can be used to log data
        in this function are: &#39;all_cluster_steps&#39;, &#39;BGM_width&#39;, &#39;BGM_height&#39;,
        &#39;snippet_clusters&#39;, &#39;eod_deletion&#39; (see extract_pulsefish()).

    Returns
    -------
    labels : list of ints
        EOD cluster labels based on height and EOD waveform.
    x_merge : list of ints
        Locations of EODs in clusters.
    saved_data : dictionary
        Key value pairs of logged data. Data to be logged is specified
        by return_data.

    &#34;&#34;&#34;
    saved_data = {}

    if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
        all_heightlabels = []
        all_shapelabels = []
        all_snippets = []
        all_features = []
        all_heights = []
        all_unique_heightlabels = []

    all_p_clusters = np.ones(len(eod_xp))*-1
    all_t_clusters = np.ones(len(eod_xp))*-1
    artefact_masks_p = np.ones(len(eod_xp), dtype=bool)
    artefact_masks_t = np.ones(len(eod_xp), dtype=bool)

    x_merge = np.ones(len(eod_xp))*-1

    max_label_p = 0   # keep track of the labels so that no labels are overwritten
    max_label_t = 0

    # loop only over height clusters that are bigger than minp
    # first cluster on width
    width_labels, bgm_log_dict = BGM(1000*eod_widths/samplerate, merge_threshold_width,
                                     n_gaus_width, use_log=False, verbose=verbose-1,
                                     plot_level=plot_level-1, xlabel=&#39;width [ms]&#39;,
                                     save_plot=save_plots, save_path=save_path,
                                     save_name=&#39;width&#39;, ftype=ftype, return_data=return_data)
    saved_data.update(bgm_log_dict)

    if verbose&gt;0:
        print(&#39;Clusters generated based on EOD width:&#39;)
        [print(&#39;N_{} = {:&gt;4}      h_{} = {:.4f}&#39;.format(l, len(width_labels[width_labels==l]), l, np.mean(eod_widths[width_labels==l]))) for l in np.unique(width_labels)]   


    w_labels, w_counts = unique_counts(width_labels)
    unique_width_labels = w_labels[w_counts&gt;minp]

    for wi, width_label in enumerate(unique_width_labels):

        # select only features in one width cluster at a time
        w_eod_widths = eod_widths[width_labels==width_label]
        w_eod_heights = eod_heights[width_labels==width_label]
        w_eod_xp = eod_xp[width_labels==width_label]
        w_eod_xt = eod_xt[width_labels==width_label]
        wp_clusters = np.ones(len(w_eod_xp))*-1
        wt_clusters = np.ones(len(w_eod_xp))*-1
        wartefact_mask = np.ones(len(w_eod_xp))

        # determine height labels
        raw_p_snippets, p_snippets, p_features, p_bg_ratio = \
          extract_snippet_features(data, w_eod_xp, w_eod_widths, w_eod_heights,
                                   width_factor_shape)
        raw_t_snippets, t_snippets, t_features, t_bg_ratio = \
          extract_snippet_features(data, w_eod_xt, w_eod_widths, w_eod_heights,
                                   width_factor_shape)

        height_labels, bgm_log_dict = \
          BGM(w_eod_heights,
              min(merge_threshold_height, np.median(np.min(np.vstack([p_bg_ratio, t_bg_ratio]), axis=0))),
              n_gaus_height, use_log=True, verbose=verbose-1, plot_level=plot_level-1,
              xlabel = &#39;height [a.u.]&#39;, save_plot=save_plots, save_path=save_path,
              save_name = &#39;height_%d&#39; % wi, ftype=ftype, return_data=return_data)
        saved_data.update(bgm_log_dict)

        if verbose&gt;0:
            print(&#39;Clusters generated based on EOD height:&#39;)
            [print(&#39;N_{} = {:&gt;4}      h_{} = {:.4f}&#39;.format(l, len(height_labels[height_labels==l]), l, np.mean(w_eod_heights[height_labels==l]))) for l in np.unique(height_labels)]   

        h_labels, h_counts = unique_counts(height_labels)
        unique_height_labels = h_labels[h_counts&gt;minp]

        if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
            all_heightlabels.append(height_labels)
            all_heights.append(w_eod_heights)
            all_unique_heightlabels.append(unique_height_labels)
            shape_labels = []
            cfeatures = []
            csnippets = []

        for hi, height_label in enumerate(unique_height_labels):

            h_eod_widths = w_eod_widths[height_labels==height_label]
            h_eod_heights = w_eod_heights[height_labels==height_label]
            h_eod_xp = w_eod_xp[height_labels==height_label]
            h_eod_xt = w_eod_xt[height_labels==height_label]

            p_clusters = cluster_on_shape(p_features[height_labels==height_label],
                                          p_bg_ratio, minp, verbose=0)            
            t_clusters = cluster_on_shape(t_features[height_labels==height_label],
                                          t_bg_ratio, minp, verbose=0)            
            
            if plot_level&gt;1:
                plot_feature_extraction(raw_p_snippets[height_labels==height_label],
                                        p_snippets[height_labels==height_label],
                                        p_features[height_labels==height_label],
                                        p_clusters, 1/samplerate, 0)
                plt.savefig(&#39;%sDBSCAN_peak_w%i_h%i.%s&#39; % (save_path, wi, hi, ftype))
                plot_feature_extraction(raw_t_snippets[height_labels==height_label],
                                        t_snippets[height_labels==height_label],
                                        t_features[height_labels==height_label],
                                        t_clusters, 1/samplerate, 1)
                plt.savefig(&#39;%sDBSCAN_trough_w%i_h%i.%s&#39; % (save_path, wi, hi, ftype))

            if &#39;snippet_clusters&#39; in return_data:
                saved_data[&#39;snippet_clusters_%i_%i_peak&#39; % (width_label, height_label)] = {
                    &#39;raw_snippets&#39;:raw_p_snippets[height_labels==height_label],
                    &#39;snippets&#39;:p_snippets[height_labels==height_label],
                    &#39;features&#39;:p_features[height_labels==height_label],
                    &#39;clusters&#39;:p_clusters,
                    &#39;samplerate&#39;:samplerate}
                saved_data[&#39;snippet_clusters_%i_%i_trough&#39; % (width_label, height_label)] = {
                    &#39;raw_snippets&#39;:raw_t_snippets[height_labels==height_label],
                    &#39;snippets&#39;:t_snippets[height_labels==height_label],
                    &#39;features&#39;:t_features[height_labels==height_label],
                    &#39;clusters&#39;:t_clusters,
                    &#39;samplerate&#39;:samplerate}

            if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
                shape_labels.append([p_clusters, t_clusters])
                cfeatures.append([p_features[height_labels==height_label],
                                  t_features[height_labels==height_label]])
                csnippets.append([p_snippets[height_labels==height_label],
                                  t_snippets[height_labels==height_label]])

            p_clusters[p_clusters==-1] = -max_label_p - 1
            wp_clusters[height_labels==height_label] = p_clusters + max_label_p
            max_label_p = max(np.max(wp_clusters), np.max(all_p_clusters)) + 1

            t_clusters[t_clusters==-1] = -max_label_t - 1
            wt_clusters[height_labels==height_label] = t_clusters + max_label_t
            max_label_t = max(np.max(wt_clusters), np.max(all_t_clusters)) + 1

        if verbose &gt; 0:
            if np.max(wp_clusters) == -1:
                print(&#39;No EOD peaks in width cluster %i&#39; % width_label)
            else:
                unique_clusters = np.unique(wp_clusters[wp_clusters!=-1])
                if len(unique_clusters) &gt; 1:
                    print(&#39;%i different EOD peaks in width cluster %i&#39; % (len(unique_clusters), width_label))
        
        if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
            all_shapelabels.append(shape_labels)
            all_snippets.append(csnippets)
            all_features.append(cfeatures)

        # for each cluster, save fft + label
        # so I end up with features for each label, and the masks.
        # then I can extract e.g. first artefact or wave etc.

        # remove artefacts here, based on the mean snippets ffts.
        artefact_masks_p[width_labels==width_label], sdict = \
          remove_artefacts(p_snippets, wp_clusters, samplerate,
                           verbose=verbose-1, return_data=return_data)
        saved_data.update(sdict)
        artefact_masks_t[width_labels==width_label], _ = \
          remove_artefacts(t_snippets, wt_clusters, samplerate,
                           verbose=verbose-1, return_data=return_data)

        # update maxlab so that no clusters are overwritten
        all_p_clusters[width_labels==width_label] = wp_clusters
        all_t_clusters[width_labels==width_label] = wt_clusters
    
    # remove all non-reliable clusters
    unreliable_fish_mask_p, saved_data = \
      delete_unreliable_fish(all_p_clusters, eod_widths, eod_xp,
                             verbose=verbose-1, sdict=saved_data)
    unreliable_fish_mask_t, _ = \
      delete_unreliable_fish(all_t_clusters, eod_widths, eod_xt, verbose=verbose-1)
    
    wave_mask_p, sidepeak_mask_p, saved_data = \
      delete_wavefish_and_sidepeaks(data, all_p_clusters, eod_xp, eod_widths,
                                    width_factor_wave, verbose=verbose-1, sdict=saved_data)
    wave_mask_t, sidepeak_mask_t, _ = \
      delete_wavefish_and_sidepeaks(data, all_t_clusters, eod_xt, eod_widths,
                                    width_factor_wave, verbose=verbose-1)  
        
    og_clusters = [np.copy(all_p_clusters), np.copy(all_t_clusters)]
    og_labels=np.copy(all_p_clusters+all_t_clusters)

    # go through all clusters and masks??
    all_p_clusters[(artefact_masks_p | unreliable_fish_mask_p | wave_mask_p | sidepeak_mask_p)] = -1
    all_t_clusters[(artefact_masks_t | unreliable_fish_mask_t | wave_mask_t | sidepeak_mask_t)] = -1

    # merge here.
    all_clusters, x_merge, mask = merge_clusters(np.copy(all_p_clusters),
                                                 np.copy(all_t_clusters), eod_xp, eod_xt,
                                                 verbose=verbose-1)
    
    if &#39;all_cluster_steps&#39; in return_data or plot_level&gt;0:
        all_dmasks = []
        all_mmasks = []

        discarding_masks = \
          np.vstack(((artefact_masks_p | unreliable_fish_mask_p | wave_mask_p | sidepeak_mask_p),
                     (artefact_masks_t | unreliable_fish_mask_t | wave_mask_t | sidepeak_mask_t)))
        merge_mask = mask

        # save the masks in the same formats as the snippets
        for wi, (width_label, w_shape_label, heightlabels, unique_height_labels) in enumerate(zip(unique_width_labels, all_shapelabels, all_heightlabels, all_unique_heightlabels)):
            w_dmasks = discarding_masks[:,width_labels==width_label]
            w_mmasks = merge_mask[:,width_labels==width_label]

            wd_2 = []
            wm_2 = []

            for hi, (height_label, h_shape_label) in enumerate(zip(unique_height_labels, w_shape_label)):
               
                h_dmasks = w_dmasks[:,heightlabels==height_label]
                h_mmasks = w_mmasks[:,heightlabels==height_label]

                wd_2.append(h_dmasks)
                wm_2.append(h_mmasks)

            all_dmasks.append(wd_2)
            all_mmasks.append(wm_2)

        if plot_level&gt;0:
            plot_clustering(samplerate, [unique_width_labels, eod_widths, width_labels],
                            [all_unique_heightlabels, all_heights, all_heightlabels],
                            [all_snippets, all_features, all_shapelabels],
                            all_dmasks, all_mmasks)
            if save_plots:
                plt.savefig(&#39;%sclustering.%s&#39; % (save_path, ftype))

        if &#39;all_cluster_steps&#39; in return_data:
            saved_data = {&#39;samplerate&#39;: samplerate,
                      &#39;EOD_widths&#39;: [unique_width_labels, eod_widths, width_labels],
                      &#39;EOD_heights&#39;: [all_unique_heightlabels, all_heights, all_heightlabels],
                      &#39;EOD_shapes&#39;: [all_snippets, all_features, all_shapelabels],
                      &#39;discarding_masks&#39;: all_dmasks,
                      &#39;merge_masks&#39;: all_mmasks
                    }
    
    if &#39;masks&#39; in return_data:
        saved_data = {&#39;masks&#39; : np.vstack(((artefact_masks_p &amp; artefact_masks_t),
                                           (unreliable_fish_mask_p &amp; unreliable_fish_mask_t),
                                           (wave_mask_p &amp; wave_mask_t),
                                           (sidepeak_mask_p &amp; sidepeak_mask_t),
                                           (all_p_clusters+all_t_clusters)))}

    if verbose&gt;0:
        print(&#39;Clusters generated based on height, width and shape: &#39;)
        [print(&#39;N_{} = {:&gt;4}&#39;.format(int(l), len(all_clusters[all_clusters == l]))) for l in np.unique(all_clusters[all_clusters != -1])]
             
    return all_clusters, x_merge, saved_data


def BGM(x, merge_threshold=0.1, n_gaus=5, max_iter=200, n_init=5,
        use_log=False, verbose=0, plot_level=0, xlabel=&#39;x [a.u.]&#39;,
        save_plot=False, save_path=&#39;&#39;, save_name=&#39;&#39;, ftype=&#39;pdf&#39;, return_data=[]):
    &#34;&#34;&#34; Use a Bayesian Gaussian Mixture Model to cluster one-dimensional data.

    Additional steps are used to merge clusters that are closer than
    `merge_threshold`.  Broad gaussian fits that cover one or more other
    gaussian fits are split by their intersections with the other
    gaussians.

    Parameters
    ----------
    x : 1D numpy array
        Features to compute clustering on. 

    merge_threshold : float (optional)
        Ratio for merging nearby gaussians.
    n_gaus: int (optional)
        Maximum number of gaussians to fit on data.
    max_iter : int (optional)
        Maximum number of iterations for gaussian fit.
    n_init : int (optional)
        Number of initializations for the gaussian fit.
    use_log: boolean (optional)
        Set to True to compute the gaussian fit on the logarithm of x.
        Can improve clustering on features with nonlinear relationships such as peak height.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    xlabel : string (optional)
        Xlabel for displaying BGM plot.
    save_plot : bool (optional)
        Set to True to save created plot.
    save_path : string (optional)
        Path to location where data should be saved. Only used if save_plot==True.
    save_name : string (optional)
        Filename of the saved plot. Usefull as usually multiple BGM models are generated.
    ftype : string (optional)
        Filetype of plot image if save_plots==True.
    return_data : list of strings (optional)
        Keys that specify data to be logged. Keys that can be used to log data
        in this function are: &#39;BGM_width&#39; and/or &#39;BGM_height&#39; (see extract_pulsefish()).

    Returns
    -------
    labels : 1D numpy array
        Cluster labels for each sample in x.
    bgm_dict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;

    bgm_dict = {}

    if len(np.unique(x)) &gt; n_gaus:
        BGM_model = BayesianGaussianMixture(n_gaus, max_iter=max_iter, n_init=n_init)
        if use_log:
            labels = BGM_model.fit_predict(stats.zscore(np.log(x)).reshape(-1, 1))
        else:
            labels = BGM_model.fit_predict(stats.zscore(x).reshape(-1, 1))
    else:
        return np.zeros(len(x)), bgm_dict
    
    if verbose&gt;0:
        if not BGM_model.converged_:
            print(&#39;!!! Gaussian mixture did not converge !!!&#39;)
    
    cur_labels = np.unique(labels)
    
    # map labels to be increasing for increasing values for x
    maxlab = len(cur_labels)
    aso = np.argsort([np.median(x[labels == l]) for l in cur_labels]) + 100
    for i, a in zip(cur_labels, aso):
        labels[labels==i] = a
    labels = labels - 100
    
    # separate gaussian clusters that can be split by other clusters
    splits = np.sort(np.copy(x))[1:][np.diff(labels[np.argsort(x)])!=0]

    labels[:] = 0
    for i, split in enumerate(splits):
        labels[x&gt;=split] = i+1

    labels_before_merge = np.copy(labels)

    # merge gaussian clusters that are closer than merge_threshold
    labels = merge_gaussians(x, labels, merge_threshold)

    if &#39;BGM_&#39;+save_name.split(&#39;_&#39;)[0] in return_data or plot_level&gt;0:

        #sort model attributes by model_means_
        means = [m[0] for m in BGM_model.means_]
        weights = [w for w in BGM_model.weights_]
        variances = [v[0][0] for v in BGM_model.covariances_]
        weights = [w for _, w in sorted(zip(means, weights))]
        variances =  [v for _, v in sorted(zip(means, variances))]
        means =  sorted(means)
        
        if plot_level&gt;0:
            plot_bgm(x, means, variances, weights, use_log, labels_before_merge,
                     labels, xlabel)
            if save_plot:
                plt.savefig(&#39;%sBGM_%s.%s&#39; % (save_path, save_name, ftype))
    
        if &#39;BGM_&#39;+save_name.split(&#39;_&#39;)[0] in return_data:
            bgm_dict[&#39;BGM_&#39;+save_name] = {&#39;x&#39;:x,
                                &#39;use_log&#39;:use_log,
                                &#39;BGM&#39;:[weights, means, variances],
                                &#39;labels&#39;:labels_before_merge,
                                &#39;xlab&#39;:xlabel}

    return labels, bgm_dict


def merge_gaussians(x, labels, merge_threshold=0.1):
    &#34;&#34;&#34; Merge all clusters which have medians which are near. Only works in 1D.

    Parameters
    ----------
    x : 1D array of ints or floats
        Features used for clustering.
    labels : 1D array of ints
        Labels for each sample in x.
    merge_threshold : float (optional)
        Similarity threshold to merge clusters.

    Returns
    -------
    labels : 1D array of ints
        Merged labels for each sample in x.
    &#34;&#34;&#34;

    # compare all the means of the gaussians. If they are too close, merge them.
    unique_labels = np.unique(labels[labels!=-1])
    x_medians = [np.median(x[labels==l]) for l in unique_labels]

    # fill a dict with the label mappings
    mapping = {}
    for label_1, x_m1 in zip(unique_labels, x_medians):
        for label_2, x_m2 in zip(unique_labels, x_medians):
            if label_1!=label_2:
                if np.abs(np.diff([x_m1, x_m2]))/np.max([x_m1, x_m2]) &lt; merge_threshold:
                    mapping[label_1] = label_2
    # apply mapping
    for map_key, map_value in mapping.items():
        labels[labels==map_key] = map_value

    return labels


def extract_snippet_features(data, eod_x, eod_widths, eod_heights, width_factor, n_pc=5):
    &#34;&#34;&#34; Extract snippets from recording data, normalize them, and perform PCA.

    Parameters
    ----------
    data : 1D numpy array of floats
        Recording data.
    eod_x : 1D array of ints
        Locations of EODs as indices.
    eod_widths : 1D array of ints
        EOD widths in samples.
    eod_heights: 1D array of floats
        EOD heights.
    width_factor: float
        Multiplier for extracting EOD snippets        

    n_pc : int (optional)
        Number of PCs to use for PCA.

    Returns
    -------
    raw_snippets : 2D numpy array (N, EOD_width)
        Raw extracted EOD snippets.
    snippets : 2D numpy array (N, EOD_width)
        Normalized EOD snippets
    features : 2D numpy array (N,n_pc)
        PC values of EOD snippets
    bg_ratio : 1D numpy array (N)
        Ratio of the background activity slopes compared to EOD height.
    &#34;&#34;&#34;

    # extract snippets with corresponding width
    width = width_factor*np.median(eod_widths)
    raw_snippets = np.vstack([data[int(x-width):int(x+width)] for x in eod_x])

    # subtract the slope and normalize the snippets
    snippets, bg_ratio = subtract_slope(np.copy(raw_snippets), eod_heights)
    snippets = StandardScaler().fit_transform(snippets.T).T

    # scale so that the absolute integral = 1.
    snippets = (snippets.T/np.sum(np.abs(snippets), axis=1)).T

    # compute features for clustering on waveform
    features = PCA(n_pc).fit(snippets).transform(snippets)

    return raw_snippets, snippets, features, bg_ratio


def cluster_on_shape(features, bg_ratio, minp, percentile=80, max_epsilon=0.01,
                     slope_ratio_factor=4, min_cluster_fraction=0.01, verbose=0):
    &#34;&#34;&#34;Separate EODs by their shape using DBSCAN.

    Parameters
    ----------
    features : 2D numpy array of floats (N, n_pc)
        PCA features of each EOD in a recording.
    bg_ratio : 1D array of floats
        Ratio of background activity slope the EOD is superimposed on.
    minp : int
        Minimum number of points for core cluster (DBSCAN).

    percentile : int (optional)
        Percentile of KNN distribution, where K=minp, to use as epsilon for DBSCAN.
    max_epsilon : float (optional)
        Maximum epsilon to use for DBSCAN clustering. This is used to avoid adding
        noisy clusters.
    slope_ratio_factor : float (optional)
        Influence of the slope-to-EOD ratio on the epsilon parameter.
        A slope_ratio_factor of 4 means that slope-to-EOD ratios &gt;1/4
        start influencing epsilon.
    min_cluster_fraction : float (optional)
        Minimum fraction of all eveluated datapoint that can form a single cluster.
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    labels : 1D array of ints
        Merged labels for each sample in x.
    &#34;&#34;&#34;

    # determine clustering threshold from data
    minpc = max(minp, int(len(features)*min_cluster_fraction))  
    knn = np.sort(pairwise_distances(features, features), axis=0)[minpc]
    eps = min(max(1, slope_ratio_factor*np.median(bg_ratio))*max_epsilon,
              np.percentile(knn, percentile))

    if verbose&gt;1:
        print(&#39;epsilon = %f&#39;%eps)
        print(&#39;Slope to EOD ratio = %f&#39;%np.median(bg_ratio))

    # cluster on EOD shape
    return DBSCAN(eps=eps, min_samples=minpc).fit(features).labels_


def subtract_slope(snippets, heights):
    &#34;&#34;&#34; Subtract underlying slope from all EOD snippets.

    Parameters
    ----------
    snippets: 2-D numpy array
        All EODs in a recorded stacked as snippets. 
        Shape = (number of EODs, EOD width)
    heights: 1D numpy array
        EOD heights.

    Returns
    -------
    snippets: 2-D numpy array
        EOD snippets with underlying slope subtracted.
    bg_ratio : 1-D numpy array
        EOD height/background activity height.
    &#34;&#34;&#34;

    left_y = snippets[:,0]
    right_y = snippets[:,-1]

    try:
        slopes = np.linspace(left_y, right_y, snippets.shape[1])
    except ValueError:
        delta = (right_y - left_y)/snippets.shape[1]
        slopes = np.arange(0, snippets.shape[1], dtype=snippets.dtype).reshape((-1,) + (1,) * np.ndim(delta))*delta + left_y
    
    return snippets - slopes.T, np.abs(left_y-right_y)/heights


def remove_artefacts(all_snippets, clusters, samplerate,
                     freq_low=20000, threshold=0.75,
                     verbose=0, return_data=[]):
    &#34;&#34;&#34; Create a mask for EOD clusters that result from artefacts, based on power in low frequency spectrum.

    Parameters
    ----------
    all_snippets: 2D array
        EOD snippets. Shape=(nEODs, EOD length)
    clusters: list of ints
        EOD cluster labels
    samplerate : float
        Samplerate of original recording data.
    freq_low: float
        Frequency up to which low frequency components are summed up. 
    threshold : float (optional)
        Minimum value for sum of low frequency components relative to
        sum overa ll spectrl amplitudes that separates artefact from
        clean pulsefish clusters.
    verbose : int (optional)
        Verbosity level.
    return_data : list of strings (optional)
        Keys that specify data to be logged. The key that can be used to log data in this function is
        &#39;eod_deletion&#39; (see extract_pulsefish()).

    Returns
    -------
    mask: numpy array of booleans
        Set to True for every EOD which is an artefact.
    adict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;
    adict = {}

    mask = np.zeros(clusters.shape, dtype=bool)

    for cluster in np.unique(clusters[clusters &gt;= 0]):
        snippets = all_snippets[clusters == cluster]
        mean_eod = np.mean(snippets, axis=0)
        mean_eod = mean_eod - np.mean(mean_eod)
        mean_eod_fft = np.abs(np.fft.rfft(mean_eod))
        freqs = np.fft.rfftfreq(len(mean_eod), 1/samplerate)
        low_frequency_ratio = np.sum(mean_eod_fft[freqs&lt;freq_low])/np.sum(mean_eod_fft)
        if low_frequency_ratio &lt; threshold:  # TODO: check threshold!
            mask[clusters==cluster] = True
            
            if verbose &gt; 0:
                print(&#39;Deleting cluster %i with low frequency ratio of %.3f (min %.3f)&#39; % (cluster, low_frequency_ratio, threshold))

        if &#39;eod_deletion&#39; in return_data:
            adict[&#39;vals_%d&#39; % cluster] = [mean_eod, mean_eod_fft]
            adict[&#39;mask_%d&#39; % cluster] = [np.any(mask[clusters==cluster])]
    
    return mask, adict


def delete_unreliable_fish(clusters, eod_widths, eod_x, verbose=0, sdict={}):
    &#34;&#34;&#34; Create a mask for EOD clusters that are either mixed with noise or other fish, or wavefish.
    
    This is the case when the ration between the EOD width and the ISI is too large.

    Parameters
    ----------
    clusters : list of ints
        Cluster labels.
    eod_widths : list of floats or ints
        EOD widths in samples or seconds.
    eod_x : list of ints or floats
        EOD times in samples or seconds.

    verbose : int (optional)
        Verbosity level.
    sdict : dictionary
        Dictionary that is used to log data. This is only used if a dictionary
        was created by remove_artefacts().
        For logging data in noise and wavefish discarding steps,
        see remove_artefacts().

    Returns
    -------
    mask : numpy array of booleans
        Set to True for every unreliable EOD.
    sdict : dictionary
        Key value pairs of logged data. Data is only logged if a dictionary
        was instantiated by remove_artefacts().
    &#34;&#34;&#34;
    mask = np.zeros(clusters.shape, dtype=bool)
    for cluster in np.unique(clusters[clusters &gt;= 0]):
        if len(eod_x[cluster == clusters]) &lt; 2:
            mask[clusters == cluster] = True
            if verbose&gt;0:
                print(&#39;deleting unreliable cluster %i, number of EOD times %d &lt; 2&#39; % (cluster, len(eod_x[cluster==clusters])))
        elif np.max(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters])) &gt; 0.5:
            if verbose&gt;0:
                print(&#39;deleting unreliable cluster %i, score=%f&#39; % (cluster, np.max(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters]))))
            mask[clusters==cluster] = True
        if &#39;vals_%d&#39; % cluster in sdict:
            sdict[&#39;vals_%d&#39; % cluster].append(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters]))
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask[clusters==cluster]))
    return mask, sdict


def delete_wavefish_and_sidepeaks(data, clusters, eod_x, eod_widths,
                                  width_fac, max_slope_deviation=0.5,
                                  max_phases=4, verbose=0, sdict={}):
    &#34;&#34;&#34; Create a mask for EODs that are likely from wavefish, or sidepeaks of bigger EODs.

    Parameters
    ----------
    data : list of floats
        Raw recording data.
    clusters : list of ints
        Cluster labels.
    eod_x : list of ints
        Indices of EOD times.
    eod_widths : list of ints
        EOD widths in samples.
    width_fac : float
        Multiplier for EOD analysis width.

    max_slope_deviation: float (optional)
        Maximum deviation of position of maximum slope in snippets from
        center position in multiples of mean width of EOD.
    max_phases : int (optional)
        Maximum number of phases for any EOD. 
        If the mean EOD has more phases than this, it is not a pulse EOD.
    verbose : int (optional) 
        Verbosity level.
    sdict : dictionary
        Dictionary that is used to log data. This is only used if a dictionary
        was created by remove_artefacts().
        For logging data in noise and wavefish discarding steps, see remove_artefacts().

    Returns
    -------
    mask_wave: numpy array of booleans
        Set to True for every EOD which is a wavefish EOD.
    mask_sidepeak: numpy array of booleans
        Set to True for every snippet which is centered around a sidepeak of an EOD.
    sdict : dictionary
        Key value pairs of logged data. Data is only logged if a dictionary
        was instantiated by remove_artefacts().
    &#34;&#34;&#34;
    mask_wave = np.zeros(clusters.shape, dtype=bool)
    mask_sidepeak = np.zeros(clusters.shape, dtype=bool)

    for i, cluster in enumerate(np.unique(clusters[clusters &gt;= 0])):
        mean_width = np.mean(eod_widths[clusters == cluster])
        cutwidth = mean_width*width_fac
        current_x = eod_x[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
        current_clusters = clusters[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
        snippets = np.vstack([data[int(x-cutwidth):int(x+cutwidth)]
                              for x in current_x[current_clusters==cluster]])
        
        # extract information on main peaks and troughs:
        mean_eod = np.mean(snippets, axis=0)
        mean_eod = mean_eod - np.mean(mean_eod)

        # detect peaks and troughs on data + some maxima/minima at the
        # end, so that the sides are also considered for peak detection:
        pk, tr = detect_peaks(np.concatenate(([-10*mean_eod[0]], mean_eod, [10*mean_eod[-1]])),
                              np.std(mean_eod))
        pk = pk[(pk&gt;0)&amp;(pk&lt;len(mean_eod))]
        tr = tr[(tr&gt;0)&amp;(tr&lt;len(mean_eod))]

        if len(pk)&gt;0 and len(tr)&gt;0:
            idxs = np.sort(np.concatenate((pk, tr)))
            slopes = np.abs(np.diff(mean_eod[idxs]))
            m_slope = np.argmax(slopes)
            centered = np.min(np.abs(idxs[m_slope:m_slope+2] - len(mean_eod)//2))
            
            # compute all height differences of peaks and troughs within snippets.
            # if they are all similar, it is probably noise or a wavefish.
            idxs = np.sort(np.concatenate((pk, tr)))
            hdiffs = np.diff(mean_eod[idxs])

            if centered &gt; max_slope_deviation*mean_width:  # TODO: check, factor was probably 0.16
                if verbose &gt; 0:
                    print(&#39;Deleting cluster %i, which is a sidepeak&#39; % cluster)
                mask_sidepeak[clusters==cluster] = True

            w_diff = np.abs(np.diff(np.sort(np.concatenate((pk, tr)))))

            if np.abs(np.diff(idxs[m_slope:m_slope+2])) &lt; np.mean(eod_widths[clusters==cluster])*0.5 or len(pk) + len(tr)&gt;max_phases or np.min(w_diff)&gt;2*cutwidth/width_fac: #or len(hdiffs[np.abs(hdiffs)&gt;0.5*(np.max(mean_eod)-np.min(mean_eod))])&gt;max_phases:
                if verbose&gt;0:
                    print(&#39;Deleting cluster %i, which is a wavefish&#39; % cluster)
                mask_wave[clusters==cluster] = True
        if &#39;vals_%d&#39; % cluster in sdict:
            sdict[&#39;vals_%d&#39; % cluster].append([mean_eod, [pk, tr],
                                               idxs[m_slope:m_slope+2]])
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask_wave[clusters==cluster]))
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask_sidepeak[clusters==cluster]))

    return mask_wave, mask_sidepeak, sdict


def merge_clusters(clusters_1, clusters_2, x_1, x_2, verbose=0): 
    &#34;&#34;&#34; Merge clusters resulting from two clustering methods.

    This method only works  if clustering is performed on the same EODs
    with the same ordering, where there  is a one to one mapping from
    clusters_1 to clusters_2. 

    Parameters
    ----------
    clusters_1: list of ints
        EOD cluster labels for cluster method 1.
    clusters_2: list of ints
        EOD cluster labels for cluster method 2.
    x_1: list of ints
        Indices of EODs for cluster method 1 (clusters_1).
    x_2: list of ints
        Indices of EODs for cluster method 2 (clusters_2).
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    clusters : list of ints
        Merged clusters.
    x_merged : list of ints
        Merged cluster indices.
    mask : 2d numpy array of ints (N, 2)
        Mask for clusters that are selected from clusters_1 (mask[:,0]) and
        from clusters_2 (mask[:,1]).
    &#34;&#34;&#34;
    if verbose &gt; 0:
        print(&#39;\nMerge cluster:&#39;)

    # these arrays become 1 for each EOD that is chosen from that array
    c1_keep = np.zeros(len(clusters_1))
    c2_keep = np.zeros(len(clusters_2))

    # add n to one of the cluster lists to avoid overlap
    ovl = np.max(clusters_1) + 1
    clusters_2[clusters_2!=-1] = clusters_2[clusters_2!=-1] + ovl

    remove_clusters = [[]]
    keep_clusters = []
    og_clusters = [np.copy(clusters_1), np.copy(clusters_2)]
    
    # loop untill done
    while True:

        # compute unique clusters and cluster sizes
        # of cluster that have not been iterated over:
        c1_labels, c1_size = unique_counts(clusters_1[(clusters_1 != -1) &amp; (c1_keep == 0)])
        c2_labels, c2_size = unique_counts(clusters_2[(clusters_2 != -1) &amp; (c2_keep == 0)])

        # if all clusters are done, break from loop:
        if len(c1_size) == 0 and len(c2_size) == 0:
            break

        # if the biggest cluster is in c_p, keep this one and discard all clusters
        # on the same indices in c_t:
        elif np.argmax([np.max(np.append(c1_size, 0)), np.max(np.append(c2_size, 0))]) == 0:
            
            # remove all the mappings from the other indices
            cluster_mappings, _ = unique_counts(clusters_2[clusters_1 == c1_labels[np.argmax(c1_size)]])
            
            clusters_2[np.isin(clusters_2, cluster_mappings)] = -1
            
            c1_keep[clusters_1==c1_labels[np.argmax(c1_size)]] = 1

            remove_clusters.append(cluster_mappings)
            keep_clusters.append(c1_labels[np.argmax(c1_size)])

            if verbose &gt; 0:
                print(&#39;Keep cluster %i of group 1, delete clusters %s of group 2&#39; % (c1_labels[np.argmax(c1_size)], str(cluster_mappings[cluster_mappings!=-1] - ovl)))

        # if the biggest cluster is in c_t, keep this one and discard all mappings in c_p
        elif np.argmax([np.max(np.append(c1_size, 0)), np.max(np.append(c2_size, 0))]) == 1:
            
            # remove all the mappings from the other indices
            cluster_mappings, _ = unique_counts(clusters_1[clusters_2 == c2_labels[np.argmax(c2_size)]])
            
            clusters_1[np.isin(clusters_1, cluster_mappings)] = -1

            c2_keep[clusters_2==c2_labels[np.argmax(c2_size)]] = 1

            remove_clusters.append(cluster_mappings)
            keep_clusters.append(c2_labels[np.argmax(c2_size)])

            if verbose &gt; 0:
                print(&#39;Keep cluster %i of group 2, delete clusters %s of group 1&#39; % (c2_labels[np.argmax(c2_size)] - ovl, str(cluster_mappings[cluster_mappings!=-1])))
    
    # combine results    
    clusters = (clusters_1+1)*c1_keep + (clusters_2+1)*c2_keep - 1
    x_merged = (x_1)*c1_keep + (x_2)*c2_keep

    return clusters, x_merged, np.vstack([c1_keep, c2_keep])


def extract_means(data, eod_x, eod_peak_x, eod_tr_x, eod_widths, clusters, samplerate,
                  width_fac, verbose=0):
    &#34;&#34;&#34; Extract mean EODs and EOD timepoints for each EOD cluster.

    Parameters
    ----------
    data: list of floats
        Raw recording data.
    eod_x: list of ints
        Locations of EODs in samples.
    eod_peak_x : list of ints
        Locations of EOD peaks in samples.
    eod_tr_x : list of ints
        Locations of EOD troughs in samples.
    eod_widths: list of ints
        EOD widths in samples.
    clusters: list of ints
        EOD cluster labels
    samplerate: float
        samplerate of recording  
    width_fac : float
        Multiplication factor for window used to extract EOD.
    
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    mean_eods: list of 2D arrays (3, eod_length)
        The average EOD for each detected fish. First column is time in seconds,
        second column the mean eod, third column the standard error.
    eod_times: list of 1D arrays
        For each detected fish the times of EOD in seconds.
    eod_peak_times: list of 1D arrays
        For each detected fish the times of EOD peaks in seconds.
    eod_trough_times: list of 1D arrays
        For each detected fish the times of EOD troughs in seconds.
    eod_labels: list of ints
        Cluster label for each detected fish.
    &#34;&#34;&#34;
    mean_eods, eod_times, eod_peak_times, eod_tr_times, eod_heights, cluster_labels = [], [], [], [], [], []

    for cluster in np.unique(clusters):
        if cluster!=-1:
            cutwidth = np.mean(eod_widths[clusters==cluster])*width_fac
            current_x = eod_x[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
            current_clusters = clusters[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]

            snippets = np.vstack([data[int(x-cutwidth):int(x+cutwidth)] for x in current_x[current_clusters==cluster]])
            mean_eod = np.mean(snippets, axis=0)
            eod_time = np.arange(len(mean_eod))/samplerate - cutwidth/samplerate

            mean_eod = np.vstack([eod_time, mean_eod, np.std(snippets, axis=0)])

            mean_eods.append(mean_eod)
            eod_times.append(eod_x[clusters==cluster]/samplerate)
            eod_heights.append(np.min(mean_eod)-np.max(mean_eod))
            eod_peak_times.append(eod_peak_x[clusters==cluster]/samplerate)
            eod_tr_times.append(eod_tr_x[clusters==cluster]/samplerate)
            cluster_labels.append(cluster)
           
    return [m for _, m in sorted(zip(eod_heights, mean_eods))], [t for _, t in sorted(zip(eod_heights, eod_times))], [pt for _, pt in sorted(zip(eod_heights, eod_peak_times))], [tt for _, tt in sorted(zip(eod_heights, eod_tr_times))], [c for _, c in sorted(zip(eod_heights, cluster_labels))]


def find_clipped_clusters(clusters, mean_eods, eod_times, eod_peaktimes, eod_troughtimes,
                          cluster_labels, width_factor, clip_threshold=0.9, verbose=0):
    &#34;&#34;&#34; Detect EODs that are clipped and set all clusterlabels of these clipped EODs to -1.
                          
    Also return the mean EODs and timepoints of these clipped EODs.

    Parameters
    ----------
    clusters: array of ints
        Cluster labels for each EOD in a recording.
    mean_eods: list of numpy arrays
        Mean EOD waveform for each cluster.
    eod_times: list of numpy arrays
        EOD timepoints for each EOD cluster.
    eod_peaktimes
        EOD peaktimes for each EOD cluster.
    eod_troughtimes
        EOD troughtimes for each EOD cluster.
    cluster_labels: numpy array
        Unique EOD clusterlabels.
    clip_threshold: float
        Threshold for detecting clipped EODs.
    
    verbose: int
        Verbosity level.

    Returns
    -------
    clusters : array of ints
        Cluster labels for each EOD in the recording, where clipped EODs have been set to -1.
    clipped_eods : list of numpy arrays
        Mean EOD waveforms for each clipped EOD cluster.
    clipped_times : list of numpy arrays
        EOD timepoints for each clipped EOD cluster.
    clipped_peaktimes : list of numpy arrays
        EOD peaktimes for each clipped EOD cluster.
    clipped_troughtimes : list of numpy arrays
        EOD troughtimes for each clipped EOD cluster.
    &#34;&#34;&#34;
    clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes, clipped_labels = [], [], [], [], []

    for mean_eod, eod_time, eod_peaktime, eod_troughtime,label in zip(mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels):
        
        if (np.count_nonzero(mean_eod[1]&gt;clip_threshold) &gt; len(mean_eod[1])/(width_factor*2)) or (np.count_nonzero(mean_eod[1] &lt; -clip_threshold) &gt; len(mean_eod[1])/(width_factor*2)):
            clipped_eods.append(mean_eod)
            clipped_times.append(eod_time)
            clipped_peaktimes.append(eod_peaktime)
            clipped_troughtimes.append(eod_troughtime)
            clipped_labels.append(label)
            if verbose&gt;0:
                print(&#39;clipped pulsefish&#39;)

    clusters[np.isin(clusters, clipped_labels)] = -1

    return clusters, clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes


def delete_moving_fish(clusters, eod_t, T, eod_heights, eod_widths, samplerate,
                       min_dt=0.25, stepsize=0.05, sliding_window_factor=2000,
                       verbose=0, plot_level=0, save_plot=False, save_path=&#39;&#39;,
                       ftype=&#39;pdf&#39;, return_data=[]):
    &#34;&#34;&#34;
    Use a sliding window to detect the minimum number of fish detected simultaneously, 
    then delete all other EOD clusters. 

    Do this only for EODs within the same width clusters, as a
    moving fish will preserve its EOD width.

    Parameters
    ----------
    clusters: list of ints
        EOD cluster labels.
    eod_t: list of floats
        Timepoints of the EODs (in seconds).
    T: float
        Length of recording (in seconds).
    eod_heights: list of floats
        EOD amplitudes.
    eod_widths: list of floats
        EOD widths (in seconds).
    samplerate: float
        Recording data samplerate.

    min_dt : float (optional)
        Minimum sliding window size (in seconds).
    stepsize : float (optional)
        Sliding window stepsize (in seconds).
    sliding_window_factor : float
        Multiplier for sliding window width,
        where the sliding window width = median(EOD_width)*sliding_window_factor.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plot : bool (optional)
        Set to True to save the plots created by plot_level.
    save_path : string (optional)
        Path to save data to. Only important if you wish to save data (save_data==True).
    ftype : string (optional)
        Define the filetype to save the plots in if save_plots is set to True.
        Options are: &#39;png&#39;, &#39;jpg&#39;, &#39;svg&#39; ...
    return_data : list of strings (optional)
        Keys that specify data to be logged. The key that can be used to log data
        in this function is &#39;moving_fish&#39; (see extract_pulsefish()).

    Returns
    -------
    clusters : list of ints
        Cluster labels, where deleted clusters have been set to -1.
    window : list of 2 floats
        Start and end of window selected for deleting moving fish in seconds.
    mf_dict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;
    mf_dict = {}
    
    if len(np.unique(clusters[clusters != -1])) == 0:
        return clusters, [0, 1], {}

    all_keep_clusters = []
    width_classes = merge_gaussians(eod_widths, np.copy(clusters), 0.75)   

    all_windows = []
    all_dts = []
    ev_num = 0
    for iw, w in enumerate(np.unique(width_classes[clusters &gt;= 0])):
        # initialize variables
        min_clusters = 100
        average_height = 0
        sparse_clusters = 100
        keep_clusters = []

        dt = max(min_dt, np.median(eod_widths[width_classes==w])*sliding_window_factor)
        window_start = 0
        window_end = dt

        wclusters = clusters[width_classes==w]
        weod_t = eod_t[width_classes==w]
        weod_heights = eod_heights[width_classes==w]
        weod_widths = eod_widths[width_classes==w]

        all_dts.append(dt)

        if verbose&gt;0:
            print(&#39;sliding window dt = %f&#39;%dt)

        # make W dependent on width??
        ignore_steps = np.zeros(len(np.arange(0, T-dt+stepsize, stepsize)))

        for i, t in enumerate(np.arange(0, T-dt+stepsize, stepsize)):
            current_clusters = wclusters[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]
            if len(np.unique(current_clusters))==0:
                ignore_steps[i-int(dt/stepsize):i+int(dt/stepsize)] = 1
                if verbose&gt;0:
                    print(&#39;No pulsefish in recording at T=%.2f:%.2f&#39; % (t, t+dt))

        
        x = np.arange(0, T-dt+stepsize, stepsize)
        y = np.ones(len(x))

        running_sum = np.ones(len(np.arange(0, T+stepsize, stepsize)))
        ulabs = np.unique(wclusters[wclusters&gt;=0])

        # sliding window
        for j, (t, ignore_step) in enumerate(zip(x, ignore_steps)):
            current_clusters = wclusters[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]
            current_widths = weod_widths[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]

            unique_clusters = np.unique(current_clusters)
            y[j] = len(unique_clusters)

            if (len(unique_clusters) &lt;= min_clusters) and \
              (ignore_step==0) and \
              (len(unique_clusters !=1)):

                current_labels = np.isin(wclusters, unique_clusters)
                current_height = np.mean(weod_heights[current_labels])

                # compute nr of clusters that are too sparse
                clusters_after_deletion = np.unique(remove_sparse_detections(np.copy(clusters[np.isin(clusters, unique_clusters)]), samplerate*eod_widths[np.isin(clusters, unique_clusters)], samplerate, T))
                current_sparse_clusters = len(unique_clusters) - len(clusters_after_deletion[clusters_after_deletion!=-1])
               
                if current_sparse_clusters &lt;= sparse_clusters and \
                  ((current_sparse_clusters&lt;sparse_clusters) or
                   (current_height &gt; average_height) or
                   (len(unique_clusters) &lt; min_clusters)):
                    
                    keep_clusters = unique_clusters
                    min_clusters = len(unique_clusters)
                    average_height = current_height
                    window_end = t+dt
                    sparse_clusters = current_sparse_clusters

        all_keep_clusters.append(keep_clusters)
        all_windows.append(window_end)
        
        if &#39;moving_fish&#39; in return_data or plot_level&gt;0:
            if &#39;w&#39; in mf_dict:
                mf_dict[&#39;w&#39;].append(np.median(eod_widths[width_classes==w]))
                mf_dict[&#39;T&#39;] = T
                mf_dict[&#39;dt&#39;].append(dt)
                mf_dict[&#39;clusters&#39;].append(wclusters)
                mf_dict[&#39;t&#39;].append(weod_t)
                mf_dict[&#39;fishcount&#39;].append([x+0.5*(x[1]-x[0]), y])
                mf_dict[&#39;ignore_steps&#39;].append(ignore_steps)
            else:
                mf_dict[&#39;w&#39;] = [np.median(eod_widths[width_classes==w])]
                mf_dict[&#39;T&#39;] = [T]
                mf_dict[&#39;dt&#39;] = [dt]
                mf_dict[&#39;clusters&#39;] = [wclusters]
                mf_dict[&#39;t&#39;] = [weod_t]
                mf_dict[&#39;fishcount&#39;] = [[x+0.5*(x[1]-x[0]), y]]
                mf_dict[&#39;ignore_steps&#39;] = [ignore_steps]

    if verbose&gt;0:
        print(&#39;Estimated nr of pulsefish in recording: %i&#39;%len(all_keep_clusters))

    if plot_level&gt;0:
        plot_moving_fish(mf_dict[&#39;w&#39;], mf_dict[&#39;dt&#39;], mf_dict[&#39;clusters&#39;],mf_dict[&#39;t&#39;],
                         mf_dict[&#39;fishcount&#39;], T, mf_dict[&#39;ignore_steps&#39;])
        if save_plot:
            plt.savefig(&#39;%sdelete_moving_fish.%s&#39; % (save_path, ftype))
        # empty dict
        if &#39;moving_fish&#39; not in return_data:
            mf_dict = {}

    # delete all clusters that are not selected
    clusters[np.invert(np.isin(clusters, np.concatenate(all_keep_clusters)))] = -1

    return clusters, [np.max(all_windows)-np.max(all_dts), np.max(all_windows)], mf_dict


def remove_sparse_detections(clusters, eod_widths, samplerate, T,
                             min_density=0.0005, verbose=0):
    &#34;&#34;&#34; Remove all EOD clusters that are too sparse

    Parameters
    ----------
    clusters : list of ints
        Cluster labels.
    eod_widths : list of ints
        Cluster widths in samples.
    samplerate : float
        Samplerate.
    T : float
        Lenght of recording in seconds.
    min_density : float (optional)
        Minimum density for realistic EOD detections.
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    clusters : list of ints
        Cluster labels, where sparse clusters have been set to -1.
    &#34;&#34;&#34;
    for c in np.unique(clusters):
        if c!=-1:

            n = len(clusters[clusters==c])
            w = np.median(eod_widths[clusters==c])/samplerate

            if n*w &lt; T*min_density:
                if verbose&gt;0:
                    print(&#39;cluster %i is too sparse&#39;%c)
                clusters[clusters==c] = -1
    return clusters</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="thunderfish.pulses.BGM"><code class="name flex">
<span>def <span class="ident">BGM</span></span>(<span>x, merge_threshold=0.1, n_gaus=5, max_iter=200, n_init=5, use_log=False, verbose=0, plot_level=0, xlabel='x [a.u.]', save_plot=False, save_path='', save_name='', ftype='pdf', return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Use a Bayesian Gaussian Mixture Model to cluster one-dimensional data.</p>
<p>Additional steps are used to merge clusters that are closer than
<code>merge_threshold</code>.
Broad gaussian fits that cover one or more other
gaussian fits are split by their intersections with the other
gaussians.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D numpy array</code></dt>
<dd>Features to compute clustering on.</dd>
<dt><strong><code>merge_threshold</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Ratio for merging nearby gaussians.</dd>
<dt><strong><code>n_gaus</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Maximum number of gaussians to fit on data.</dd>
<dt><strong><code>max_iter</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Maximum number of iterations for gaussian fit.</dd>
<dt><strong><code>n_init</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of initializations for the gaussian fit.</dd>
<dt><strong><code>use_log</code></strong> :&ensp;<code>boolean (optional)</code></dt>
<dd>Set to True to compute the gaussian fit on the logarithm of x.
Can improve clustering on features with nonlinear relationships such as peak height.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>plot_level</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Similar to verbosity levels, but with plots.
Only set to &gt; 0 for debugging purposes.</dd>
<dt><strong><code>xlabel</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Xlabel for displaying BGM plot.</dd>
<dt><strong><code>save_plot</code></strong> :&ensp;<code>bool (optional)</code></dt>
<dd>Set to True to save created plot.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Path to location where data should be saved. Only used if save_plot==True.</dd>
<dt><strong><code>save_name</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Filename of the saved plot. Usefull as usually multiple BGM models are generated.</dd>
<dt><strong><code>ftype</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Filetype of plot image if save_plots==True.</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>Keys that specify data to be logged. Keys that can be used to log data
in this function are: 'BGM_width' and/or 'BGM_height' (see extract_pulsefish()).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>1D numpy array</code></dt>
<dd>Cluster labels for each sample in x.</dd>
<dt><strong><code>bgm_dict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data to be logged is specified by return_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def BGM(x, merge_threshold=0.1, n_gaus=5, max_iter=200, n_init=5,
        use_log=False, verbose=0, plot_level=0, xlabel=&#39;x [a.u.]&#39;,
        save_plot=False, save_path=&#39;&#39;, save_name=&#39;&#39;, ftype=&#39;pdf&#39;, return_data=[]):
    &#34;&#34;&#34; Use a Bayesian Gaussian Mixture Model to cluster one-dimensional data.

    Additional steps are used to merge clusters that are closer than
    `merge_threshold`.  Broad gaussian fits that cover one or more other
    gaussian fits are split by their intersections with the other
    gaussians.

    Parameters
    ----------
    x : 1D numpy array
        Features to compute clustering on. 

    merge_threshold : float (optional)
        Ratio for merging nearby gaussians.
    n_gaus: int (optional)
        Maximum number of gaussians to fit on data.
    max_iter : int (optional)
        Maximum number of iterations for gaussian fit.
    n_init : int (optional)
        Number of initializations for the gaussian fit.
    use_log: boolean (optional)
        Set to True to compute the gaussian fit on the logarithm of x.
        Can improve clustering on features with nonlinear relationships such as peak height.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    xlabel : string (optional)
        Xlabel for displaying BGM plot.
    save_plot : bool (optional)
        Set to True to save created plot.
    save_path : string (optional)
        Path to location where data should be saved. Only used if save_plot==True.
    save_name : string (optional)
        Filename of the saved plot. Usefull as usually multiple BGM models are generated.
    ftype : string (optional)
        Filetype of plot image if save_plots==True.
    return_data : list of strings (optional)
        Keys that specify data to be logged. Keys that can be used to log data
        in this function are: &#39;BGM_width&#39; and/or &#39;BGM_height&#39; (see extract_pulsefish()).

    Returns
    -------
    labels : 1D numpy array
        Cluster labels for each sample in x.
    bgm_dict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;

    bgm_dict = {}

    if len(np.unique(x)) &gt; n_gaus:
        BGM_model = BayesianGaussianMixture(n_gaus, max_iter=max_iter, n_init=n_init)
        if use_log:
            labels = BGM_model.fit_predict(stats.zscore(np.log(x)).reshape(-1, 1))
        else:
            labels = BGM_model.fit_predict(stats.zscore(x).reshape(-1, 1))
    else:
        return np.zeros(len(x)), bgm_dict
    
    if verbose&gt;0:
        if not BGM_model.converged_:
            print(&#39;!!! Gaussian mixture did not converge !!!&#39;)
    
    cur_labels = np.unique(labels)
    
    # map labels to be increasing for increasing values for x
    maxlab = len(cur_labels)
    aso = np.argsort([np.median(x[labels == l]) for l in cur_labels]) + 100
    for i, a in zip(cur_labels, aso):
        labels[labels==i] = a
    labels = labels - 100
    
    # separate gaussian clusters that can be split by other clusters
    splits = np.sort(np.copy(x))[1:][np.diff(labels[np.argsort(x)])!=0]

    labels[:] = 0
    for i, split in enumerate(splits):
        labels[x&gt;=split] = i+1

    labels_before_merge = np.copy(labels)

    # merge gaussian clusters that are closer than merge_threshold
    labels = merge_gaussians(x, labels, merge_threshold)

    if &#39;BGM_&#39;+save_name.split(&#39;_&#39;)[0] in return_data or plot_level&gt;0:

        #sort model attributes by model_means_
        means = [m[0] for m in BGM_model.means_]
        weights = [w for w in BGM_model.weights_]
        variances = [v[0][0] for v in BGM_model.covariances_]
        weights = [w for _, w in sorted(zip(means, weights))]
        variances =  [v for _, v in sorted(zip(means, variances))]
        means =  sorted(means)
        
        if plot_level&gt;0:
            plot_bgm(x, means, variances, weights, use_log, labels_before_merge,
                     labels, xlabel)
            if save_plot:
                plt.savefig(&#39;%sBGM_%s.%s&#39; % (save_path, save_name, ftype))
    
        if &#39;BGM_&#39;+save_name.split(&#39;_&#39;)[0] in return_data:
            bgm_dict[&#39;BGM_&#39;+save_name] = {&#39;x&#39;:x,
                                &#39;use_log&#39;:use_log,
                                &#39;BGM&#39;:[weights, means, variances],
                                &#39;labels&#39;:labels_before_merge,
                                &#39;xlab&#39;:xlabel}

    return labels, bgm_dict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.assign_side_peaks"><code class="name flex">
<span>def <span class="ident">assign_side_peaks</span></span>(<span>data, peak_indices, trough_indices, min_rel_slope_diff=0.25)</span>
</code></dt>
<dd>
<div class="desc"><p>Assign to each peak the trough resulting in a pulse with the steepest slope or largest height.</p>
<p>The slope between a peak and a trough is computed as the height
difference divided by the distance between peak and trough. If the
slopes between the left and the right trough differ by less than
<code>min_rel_slope_diff</code>, then just the heigths between and the two
troughs relative to the peak are compared.</p>
<p>Was <code>def detect_eod_peaks(data, main_indices, side_indices,
max_width=20, min_width=2, verbose=0)</code> before.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>array</code> of <code>floats</code></dt>
<dd>Data in which the events were detected.</dd>
<dt><strong><code>peak_indices</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Indices of the detected peaks in the data time series.</dd>
<dt><strong><code>trough_indices</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Indices of the detected troughs in the data time series.</dd>
<dt><strong><code>min_rel_slope_diff</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum required difference of left and right slope relative
to mean slope.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>peak_indices</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Peak indices. Same as input <code>peak_indices</code> but potentially shorter
by one or two elements.</dd>
<dt><strong><code>trough_indices</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Corresponding trough indices of trough to the left or right
of the peaks.</dd>
<dt><strong><code>heights</code></strong> :&ensp;<code>array</code> of <code>floats</code></dt>
<dd>Peak heights (distance between peak and corresponding trough amplitude)</dd>
<dt><strong><code>widths</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Peak widths (distance between peak and corresponding trough indices)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assign_side_peaks(data, peak_indices, trough_indices,
                      min_rel_slope_diff=0.25):
    &#34;&#34;&#34;Assign to each peak the trough resulting in a pulse with the steepest slope or largest height.

    The slope between a peak and a trough is computed as the height
    difference divided by the distance between peak and trough. If the
    slopes between the left and the right trough differ by less than
    `min_rel_slope_diff`, then just the heigths between and the two
    troughs relative to the peak are compared.

    Was `def detect_eod_peaks(data, main_indices, side_indices,
                              max_width=20, min_width=2, verbose=0)` before.

    Parameters
    ----------
    data: array of floats
        Data in which the events were detected.
    peak_indices: array of ints
        Indices of the detected peaks in the data time series.
    trough_indices: array of ints
        Indices of the detected troughs in the data time series. 
    min_rel_slope_diff: float
        Minimum required difference of left and right slope relative
        to mean slope.

    Returns
    -------
    peak_indices: array of ints
        Peak indices. Same as input `peak_indices` but potentially shorter
        by one or two elements.
    trough_indices: array of ints
        Corresponding trough indices of trough to the left or right
        of the peaks.
    heights: array of floats
        Peak heights (distance between peak and corresponding trough amplitude)
    widths: array of ints
        Peak widths (distance between peak and corresponding trough indices)

    &#34;&#34;&#34;
    # is a main or side peak first?
    peak_first = int(peak_indices[0] &lt; trough_indices[0])
    # is a main or side peak last?
    peak_last = int(peak_indices[-1] &gt; trough_indices[-1])
    # ensure main peaks to have side peaks at both sides:
    peak_indices = peak_indices[peak_first:len(peak_indices)-peak_last]
    y = data[peak_indices]
    
    # indices of troughs on the left and right side of main peaks:
    l_indices = np.arange(len(peak_indices))
    r_indices = l_indices + 1

    # indices, distance to peak, height, and slope of left troughs:
    l_side_indices = trough_indices[l_indices]
    l_distance = np.abs(peak_indices - l_side_indices)
    l_height = np.abs(y - data[l_side_indices])
    l_slope = np.abs(l_height/l_distance)

    # indices, distance to peak, height, and slope of right troughs:
    r_side_indices = trough_indices[r_indices]
    r_distance = np.abs(r_side_indices - peak_indices)
    r_height = np.abs(y - data[r_side_indices])
    r_slope = np.abs(r_height/r_distance)

    # which trough to assign to the peak?
    # - either the one with the steepest slope, or
    # - when slopes are similar on both sides (within 25% difference),
    #   the trough with the maximum height difference to the peak:
    rel_slopes = np.abs(l_slope-r_slope)/(0.5*(l_slope+r_slope))
    take_slopes = rel_slopes &gt; min_rel_slope_diff
    take_left = l_height &gt; r_height
    take_left[take_slopes] = l_slope[take_slopes] &gt; r_slope[take_slopes]

    # assign troughs, heights, and widths:
    trough_indices = np.where(take_left,
                              trough_indices[:-1], trough_indices[1:])
    heights = np.where(take_left, l_height, r_height)
    widths = np.where(take_left, l_distance, r_distance)
    slopes = np.where(take_left, l_slope, r_slope)

    ## TODO: check for width here?
    
    # discard connected peaks:
    same = np.nonzero(trough_indices[:-1] == trough_indices[1:])[0]
    keep = np.ones(len(trough_indices), dtype=np.bool)
    for i in same:
        # same troughs at trough_indices[i] and trough_indices[i+1]:
        s = slopes[i:i+2]
        rel_slopes = np.abs(np.diff(s))[0]/np.mean(s)
        if rel_slopes &gt; min_rel_slope_diff:
            keep[i+(s[1]&lt;s[0])] = False
        else:
            keep[i+(heights[i+1]&lt;heights[i])] = False
    
    return peak_indices[keep], trough_indices[keep], heights[keep], widths[keep]</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.cluster"><code class="name flex">
<span>def <span class="ident">cluster</span></span>(<span>eod_xp, eod_xt, eod_heights, eod_widths, data, samplerate, width_factor_shape, width_factor_wave, n_gaus_height=10, merge_threshold_height=0.1, n_gaus_width=3, merge_threshold_width=0.5, minp=10, verbose=0, plot_level=0, save_plots=False, save_path='', ftype='pdf', return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Cluster EODs.</p>
<p>First cluster on EOD widths using a Bayesian Gaussian
Mixture (BGM) model,
then cluster on EOD heights using a
BGM model. Lastly, cluster on EOD waveform with DBSCAN.
Clustering on EOD waveform is performed twice, once on
peak-centered EODs and once on trough-centered EODs.
Non-pulsetype EOD clusters are deleted, and clusters are
merged afterwards.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>eod_xp</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Location of EOD peaks in indices.</dd>
<dt><strong><code>eod_xt</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Locations of EOD troughs in indices.</dd>
<dt><strong><code>eod_heights</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>EOD heights.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD widths in samples.</dd>
<dt><strong><code>data</code></strong> :&ensp;<code>array</code> of <code>floats</code></dt>
<dd>Data in which to detect pulse EODs.</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Sample rate of <code>data</code>.</dd>
<dt><strong><code>width_factor_shape</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplier for snippet extraction width. This factor is
multiplied with the width between the peak and through of a
single EOD.</dd>
<dt><strong><code>width_factor_wave</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplier for wavefish extraction width.</dd>
<dt><strong><code>n_gaus_height</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of gaussians to use for the clustering based on EOD height.</dd>
<dt><strong><code>merge_threshold_height</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Threshold for merging clusters that are similar in height.</dd>
<dt><strong><code>n_gaus_width</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of gaussians to use for the clustering based on EOD width.</dd>
<dt><strong><code>merge_threshold_width</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Threshold for merging clusters that are similar in width.</dd>
<dt><strong><code>minp</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Minimum number of points for core clusters (DBSCAN).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>plot_level</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Similar to verbosity levels, but with plots.
Only set to &gt; 0 for debugging purposes.</dd>
<dt><strong><code>save_plots</code></strong> :&ensp;<code>bool (optional)</code></dt>
<dd>Set to True to save created plots.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Path to save plots to. Only used if save_plots==True.</dd>
<dt><strong><code>ftype</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Filetype to save plot images in.</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>Keys that specify data to be logged. Keys that can be used to log data
in this function are: 'all_cluster_steps', 'BGM_width', 'BGM_height',
'snippet_clusters', 'eod_deletion' (see extract_pulsefish()).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels based on height and EOD waveform.</dd>
<dt><strong><code>x_merge</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Locations of EODs in clusters.</dd>
<dt><strong><code>saved_data</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data to be logged is specified
by return_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster(eod_xp, eod_xt, eod_heights, eod_widths, data, samplerate,
            width_factor_shape, width_factor_wave,
            n_gaus_height=10, merge_threshold_height=0.1, n_gaus_width=3,
            merge_threshold_width=0.5, minp=10,
            verbose=0, plot_level=0, save_plots=False, save_path=&#39;&#39;, ftype=&#39;pdf&#39;,
            return_data=[]):
    &#34;&#34;&#34;Cluster EODs.
    
    First cluster on EOD widths using a Bayesian Gaussian
    Mixture (BGM) model,  then cluster on EOD heights using a
    BGM model. Lastly, cluster on EOD waveform with DBSCAN.
    Clustering on EOD waveform is performed twice, once on
    peak-centered EODs and once on trough-centered EODs.
    Non-pulsetype EOD clusters are deleted, and clusters are
    merged afterwards.

    Parameters
    ----------
    eod_xp : list of ints
        Location of EOD peaks in indices.
    eod_xt: list of ints
        Locations of EOD troughs in indices.
    eod_heights: list of floats
        EOD heights.
    eod_widths: list of ints
        EOD widths in samples.
    data: array of floats
        Data in which to detect pulse EODs.
    samplerate : float
        Sample rate of `data`.
    width_factor_shape : float
        Multiplier for snippet extraction width. This factor is
        multiplied with the width between the peak and through of a
        single EOD.
    width_factor_wave : float
        Multiplier for wavefish extraction width.
    n_gaus_height : int (optional)
        Number of gaussians to use for the clustering based on EOD height.
    merge_threshold_height : float (optional)
        Threshold for merging clusters that are similar in height.
    n_gaus_width : int (optional)
        Number of gaussians to use for the clustering based on EOD width.
    merge_threshold_width : float (optional)
        Threshold for merging clusters that are similar in width.
    minp : int (optional)
        Minimum number of points for core clusters (DBSCAN).
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plots : bool (optional)
        Set to True to save created plots.
    save_path : string (optional)
        Path to save plots to. Only used if save_plots==True.
    ftype : string (optional)
        Filetype to save plot images in.
    return_data : list of strings (optional)
        Keys that specify data to be logged. Keys that can be used to log data
        in this function are: &#39;all_cluster_steps&#39;, &#39;BGM_width&#39;, &#39;BGM_height&#39;,
        &#39;snippet_clusters&#39;, &#39;eod_deletion&#39; (see extract_pulsefish()).

    Returns
    -------
    labels : list of ints
        EOD cluster labels based on height and EOD waveform.
    x_merge : list of ints
        Locations of EODs in clusters.
    saved_data : dictionary
        Key value pairs of logged data. Data to be logged is specified
        by return_data.

    &#34;&#34;&#34;
    saved_data = {}

    if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
        all_heightlabels = []
        all_shapelabels = []
        all_snippets = []
        all_features = []
        all_heights = []
        all_unique_heightlabels = []

    all_p_clusters = np.ones(len(eod_xp))*-1
    all_t_clusters = np.ones(len(eod_xp))*-1
    artefact_masks_p = np.ones(len(eod_xp), dtype=bool)
    artefact_masks_t = np.ones(len(eod_xp), dtype=bool)

    x_merge = np.ones(len(eod_xp))*-1

    max_label_p = 0   # keep track of the labels so that no labels are overwritten
    max_label_t = 0

    # loop only over height clusters that are bigger than minp
    # first cluster on width
    width_labels, bgm_log_dict = BGM(1000*eod_widths/samplerate, merge_threshold_width,
                                     n_gaus_width, use_log=False, verbose=verbose-1,
                                     plot_level=plot_level-1, xlabel=&#39;width [ms]&#39;,
                                     save_plot=save_plots, save_path=save_path,
                                     save_name=&#39;width&#39;, ftype=ftype, return_data=return_data)
    saved_data.update(bgm_log_dict)

    if verbose&gt;0:
        print(&#39;Clusters generated based on EOD width:&#39;)
        [print(&#39;N_{} = {:&gt;4}      h_{} = {:.4f}&#39;.format(l, len(width_labels[width_labels==l]), l, np.mean(eod_widths[width_labels==l]))) for l in np.unique(width_labels)]   


    w_labels, w_counts = unique_counts(width_labels)
    unique_width_labels = w_labels[w_counts&gt;minp]

    for wi, width_label in enumerate(unique_width_labels):

        # select only features in one width cluster at a time
        w_eod_widths = eod_widths[width_labels==width_label]
        w_eod_heights = eod_heights[width_labels==width_label]
        w_eod_xp = eod_xp[width_labels==width_label]
        w_eod_xt = eod_xt[width_labels==width_label]
        wp_clusters = np.ones(len(w_eod_xp))*-1
        wt_clusters = np.ones(len(w_eod_xp))*-1
        wartefact_mask = np.ones(len(w_eod_xp))

        # determine height labels
        raw_p_snippets, p_snippets, p_features, p_bg_ratio = \
          extract_snippet_features(data, w_eod_xp, w_eod_widths, w_eod_heights,
                                   width_factor_shape)
        raw_t_snippets, t_snippets, t_features, t_bg_ratio = \
          extract_snippet_features(data, w_eod_xt, w_eod_widths, w_eod_heights,
                                   width_factor_shape)

        height_labels, bgm_log_dict = \
          BGM(w_eod_heights,
              min(merge_threshold_height, np.median(np.min(np.vstack([p_bg_ratio, t_bg_ratio]), axis=0))),
              n_gaus_height, use_log=True, verbose=verbose-1, plot_level=plot_level-1,
              xlabel = &#39;height [a.u.]&#39;, save_plot=save_plots, save_path=save_path,
              save_name = &#39;height_%d&#39; % wi, ftype=ftype, return_data=return_data)
        saved_data.update(bgm_log_dict)

        if verbose&gt;0:
            print(&#39;Clusters generated based on EOD height:&#39;)
            [print(&#39;N_{} = {:&gt;4}      h_{} = {:.4f}&#39;.format(l, len(height_labels[height_labels==l]), l, np.mean(w_eod_heights[height_labels==l]))) for l in np.unique(height_labels)]   

        h_labels, h_counts = unique_counts(height_labels)
        unique_height_labels = h_labels[h_counts&gt;minp]

        if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
            all_heightlabels.append(height_labels)
            all_heights.append(w_eod_heights)
            all_unique_heightlabels.append(unique_height_labels)
            shape_labels = []
            cfeatures = []
            csnippets = []

        for hi, height_label in enumerate(unique_height_labels):

            h_eod_widths = w_eod_widths[height_labels==height_label]
            h_eod_heights = w_eod_heights[height_labels==height_label]
            h_eod_xp = w_eod_xp[height_labels==height_label]
            h_eod_xt = w_eod_xt[height_labels==height_label]

            p_clusters = cluster_on_shape(p_features[height_labels==height_label],
                                          p_bg_ratio, minp, verbose=0)            
            t_clusters = cluster_on_shape(t_features[height_labels==height_label],
                                          t_bg_ratio, minp, verbose=0)            
            
            if plot_level&gt;1:
                plot_feature_extraction(raw_p_snippets[height_labels==height_label],
                                        p_snippets[height_labels==height_label],
                                        p_features[height_labels==height_label],
                                        p_clusters, 1/samplerate, 0)
                plt.savefig(&#39;%sDBSCAN_peak_w%i_h%i.%s&#39; % (save_path, wi, hi, ftype))
                plot_feature_extraction(raw_t_snippets[height_labels==height_label],
                                        t_snippets[height_labels==height_label],
                                        t_features[height_labels==height_label],
                                        t_clusters, 1/samplerate, 1)
                plt.savefig(&#39;%sDBSCAN_trough_w%i_h%i.%s&#39; % (save_path, wi, hi, ftype))

            if &#39;snippet_clusters&#39; in return_data:
                saved_data[&#39;snippet_clusters_%i_%i_peak&#39; % (width_label, height_label)] = {
                    &#39;raw_snippets&#39;:raw_p_snippets[height_labels==height_label],
                    &#39;snippets&#39;:p_snippets[height_labels==height_label],
                    &#39;features&#39;:p_features[height_labels==height_label],
                    &#39;clusters&#39;:p_clusters,
                    &#39;samplerate&#39;:samplerate}
                saved_data[&#39;snippet_clusters_%i_%i_trough&#39; % (width_label, height_label)] = {
                    &#39;raw_snippets&#39;:raw_t_snippets[height_labels==height_label],
                    &#39;snippets&#39;:t_snippets[height_labels==height_label],
                    &#39;features&#39;:t_features[height_labels==height_label],
                    &#39;clusters&#39;:t_clusters,
                    &#39;samplerate&#39;:samplerate}

            if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
                shape_labels.append([p_clusters, t_clusters])
                cfeatures.append([p_features[height_labels==height_label],
                                  t_features[height_labels==height_label]])
                csnippets.append([p_snippets[height_labels==height_label],
                                  t_snippets[height_labels==height_label]])

            p_clusters[p_clusters==-1] = -max_label_p - 1
            wp_clusters[height_labels==height_label] = p_clusters + max_label_p
            max_label_p = max(np.max(wp_clusters), np.max(all_p_clusters)) + 1

            t_clusters[t_clusters==-1] = -max_label_t - 1
            wt_clusters[height_labels==height_label] = t_clusters + max_label_t
            max_label_t = max(np.max(wt_clusters), np.max(all_t_clusters)) + 1

        if verbose &gt; 0:
            if np.max(wp_clusters) == -1:
                print(&#39;No EOD peaks in width cluster %i&#39; % width_label)
            else:
                unique_clusters = np.unique(wp_clusters[wp_clusters!=-1])
                if len(unique_clusters) &gt; 1:
                    print(&#39;%i different EOD peaks in width cluster %i&#39; % (len(unique_clusters), width_label))
        
        if plot_level&gt;0 or &#39;all_cluster_steps&#39; in return_data:
            all_shapelabels.append(shape_labels)
            all_snippets.append(csnippets)
            all_features.append(cfeatures)

        # for each cluster, save fft + label
        # so I end up with features for each label, and the masks.
        # then I can extract e.g. first artefact or wave etc.

        # remove artefacts here, based on the mean snippets ffts.
        artefact_masks_p[width_labels==width_label], sdict = \
          remove_artefacts(p_snippets, wp_clusters, samplerate,
                           verbose=verbose-1, return_data=return_data)
        saved_data.update(sdict)
        artefact_masks_t[width_labels==width_label], _ = \
          remove_artefacts(t_snippets, wt_clusters, samplerate,
                           verbose=verbose-1, return_data=return_data)

        # update maxlab so that no clusters are overwritten
        all_p_clusters[width_labels==width_label] = wp_clusters
        all_t_clusters[width_labels==width_label] = wt_clusters
    
    # remove all non-reliable clusters
    unreliable_fish_mask_p, saved_data = \
      delete_unreliable_fish(all_p_clusters, eod_widths, eod_xp,
                             verbose=verbose-1, sdict=saved_data)
    unreliable_fish_mask_t, _ = \
      delete_unreliable_fish(all_t_clusters, eod_widths, eod_xt, verbose=verbose-1)
    
    wave_mask_p, sidepeak_mask_p, saved_data = \
      delete_wavefish_and_sidepeaks(data, all_p_clusters, eod_xp, eod_widths,
                                    width_factor_wave, verbose=verbose-1, sdict=saved_data)
    wave_mask_t, sidepeak_mask_t, _ = \
      delete_wavefish_and_sidepeaks(data, all_t_clusters, eod_xt, eod_widths,
                                    width_factor_wave, verbose=verbose-1)  
        
    og_clusters = [np.copy(all_p_clusters), np.copy(all_t_clusters)]
    og_labels=np.copy(all_p_clusters+all_t_clusters)

    # go through all clusters and masks??
    all_p_clusters[(artefact_masks_p | unreliable_fish_mask_p | wave_mask_p | sidepeak_mask_p)] = -1
    all_t_clusters[(artefact_masks_t | unreliable_fish_mask_t | wave_mask_t | sidepeak_mask_t)] = -1

    # merge here.
    all_clusters, x_merge, mask = merge_clusters(np.copy(all_p_clusters),
                                                 np.copy(all_t_clusters), eod_xp, eod_xt,
                                                 verbose=verbose-1)
    
    if &#39;all_cluster_steps&#39; in return_data or plot_level&gt;0:
        all_dmasks = []
        all_mmasks = []

        discarding_masks = \
          np.vstack(((artefact_masks_p | unreliable_fish_mask_p | wave_mask_p | sidepeak_mask_p),
                     (artefact_masks_t | unreliable_fish_mask_t | wave_mask_t | sidepeak_mask_t)))
        merge_mask = mask

        # save the masks in the same formats as the snippets
        for wi, (width_label, w_shape_label, heightlabels, unique_height_labels) in enumerate(zip(unique_width_labels, all_shapelabels, all_heightlabels, all_unique_heightlabels)):
            w_dmasks = discarding_masks[:,width_labels==width_label]
            w_mmasks = merge_mask[:,width_labels==width_label]

            wd_2 = []
            wm_2 = []

            for hi, (height_label, h_shape_label) in enumerate(zip(unique_height_labels, w_shape_label)):
               
                h_dmasks = w_dmasks[:,heightlabels==height_label]
                h_mmasks = w_mmasks[:,heightlabels==height_label]

                wd_2.append(h_dmasks)
                wm_2.append(h_mmasks)

            all_dmasks.append(wd_2)
            all_mmasks.append(wm_2)

        if plot_level&gt;0:
            plot_clustering(samplerate, [unique_width_labels, eod_widths, width_labels],
                            [all_unique_heightlabels, all_heights, all_heightlabels],
                            [all_snippets, all_features, all_shapelabels],
                            all_dmasks, all_mmasks)
            if save_plots:
                plt.savefig(&#39;%sclustering.%s&#39; % (save_path, ftype))

        if &#39;all_cluster_steps&#39; in return_data:
            saved_data = {&#39;samplerate&#39;: samplerate,
                      &#39;EOD_widths&#39;: [unique_width_labels, eod_widths, width_labels],
                      &#39;EOD_heights&#39;: [all_unique_heightlabels, all_heights, all_heightlabels],
                      &#39;EOD_shapes&#39;: [all_snippets, all_features, all_shapelabels],
                      &#39;discarding_masks&#39;: all_dmasks,
                      &#39;merge_masks&#39;: all_mmasks
                    }
    
    if &#39;masks&#39; in return_data:
        saved_data = {&#39;masks&#39; : np.vstack(((artefact_masks_p &amp; artefact_masks_t),
                                           (unreliable_fish_mask_p &amp; unreliable_fish_mask_t),
                                           (wave_mask_p &amp; wave_mask_t),
                                           (sidepeak_mask_p &amp; sidepeak_mask_t),
                                           (all_p_clusters+all_t_clusters)))}

    if verbose&gt;0:
        print(&#39;Clusters generated based on height, width and shape: &#39;)
        [print(&#39;N_{} = {:&gt;4}&#39;.format(int(l), len(all_clusters[all_clusters == l]))) for l in np.unique(all_clusters[all_clusters != -1])]
             
    return all_clusters, x_merge, saved_data</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.cluster_on_shape"><code class="name flex">
<span>def <span class="ident">cluster_on_shape</span></span>(<span>features, bg_ratio, minp, percentile=80, max_epsilon=0.01, slope_ratio_factor=4, min_cluster_fraction=0.01, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Separate EODs by their shape using DBSCAN.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>2D numpy array</code> of <code>floats (N, n_pc)</code></dt>
<dd>PCA features of each EOD in a recording.</dd>
<dt><strong><code>bg_ratio</code></strong> :&ensp;<code>1D array</code> of <code>floats</code></dt>
<dd>Ratio of background activity slope the EOD is superimposed on.</dd>
<dt><strong><code>minp</code></strong> :&ensp;<code>int</code></dt>
<dd>Minimum number of points for core cluster (DBSCAN).</dd>
<dt><strong><code>percentile</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Percentile of KNN distribution, where K=minp, to use as epsilon for DBSCAN.</dd>
<dt><strong><code>max_epsilon</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Maximum epsilon to use for DBSCAN clustering. This is used to avoid adding
noisy clusters.</dd>
<dt><strong><code>slope_ratio_factor</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Influence of the slope-to-EOD ratio on the epsilon parameter.
A slope_ratio_factor of 4 means that slope-to-EOD ratios &gt;1/4
start influencing epsilon.</dd>
<dt><strong><code>min_cluster_fraction</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Minimum fraction of all eveluated datapoint that can form a single cluster.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>1D array</code> of <code>ints</code></dt>
<dd>Merged labels for each sample in x.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_on_shape(features, bg_ratio, minp, percentile=80, max_epsilon=0.01,
                     slope_ratio_factor=4, min_cluster_fraction=0.01, verbose=0):
    &#34;&#34;&#34;Separate EODs by their shape using DBSCAN.

    Parameters
    ----------
    features : 2D numpy array of floats (N, n_pc)
        PCA features of each EOD in a recording.
    bg_ratio : 1D array of floats
        Ratio of background activity slope the EOD is superimposed on.
    minp : int
        Minimum number of points for core cluster (DBSCAN).

    percentile : int (optional)
        Percentile of KNN distribution, where K=minp, to use as epsilon for DBSCAN.
    max_epsilon : float (optional)
        Maximum epsilon to use for DBSCAN clustering. This is used to avoid adding
        noisy clusters.
    slope_ratio_factor : float (optional)
        Influence of the slope-to-EOD ratio on the epsilon parameter.
        A slope_ratio_factor of 4 means that slope-to-EOD ratios &gt;1/4
        start influencing epsilon.
    min_cluster_fraction : float (optional)
        Minimum fraction of all eveluated datapoint that can form a single cluster.
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    labels : 1D array of ints
        Merged labels for each sample in x.
    &#34;&#34;&#34;

    # determine clustering threshold from data
    minpc = max(minp, int(len(features)*min_cluster_fraction))  
    knn = np.sort(pairwise_distances(features, features), axis=0)[minpc]
    eps = min(max(1, slope_ratio_factor*np.median(bg_ratio))*max_epsilon,
              np.percentile(knn, percentile))

    if verbose&gt;1:
        print(&#39;epsilon = %f&#39;%eps)
        print(&#39;Slope to EOD ratio = %f&#39;%np.median(bg_ratio))

    # cluster on EOD shape
    return DBSCAN(eps=eps, min_samples=minpc).fit(features).labels_</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.delete_moving_fish"><code class="name flex">
<span>def <span class="ident">delete_moving_fish</span></span>(<span>clusters, eod_t, T, eod_heights, eod_widths, samplerate, min_dt=0.25, stepsize=0.05, sliding_window_factor=2000, verbose=0, plot_level=0, save_plot=False, save_path='', ftype='pdf', return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Use a sliding window to detect the minimum number of fish detected simultaneously,
then delete all other EOD clusters. </p>
<p>Do this only for EODs within the same width clusters, as a
moving fish will preserve its EOD width.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels.</dd>
<dt><strong><code>eod_t</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>Timepoints of the EODs (in seconds).</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>float</code></dt>
<dd>Length of recording (in seconds).</dd>
<dt><strong><code>eod_heights</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>EOD amplitudes.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>EOD widths (in seconds).</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Recording data samplerate.</dd>
<dt><strong><code>min_dt</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Minimum sliding window size (in seconds).</dd>
<dt><strong><code>stepsize</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Sliding window stepsize (in seconds).</dd>
<dt><strong><code>sliding_window_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplier for sliding window width,
where the sliding window width = median(EOD_width)*sliding_window_factor.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>plot_level</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Similar to verbosity levels, but with plots.
Only set to &gt; 0 for debugging purposes.</dd>
<dt><strong><code>save_plot</code></strong> :&ensp;<code>bool (optional)</code></dt>
<dd>Set to True to save the plots created by plot_level.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Path to save data to. Only important if you wish to save data (save_data==True).</dd>
<dt><strong><code>ftype</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Define the filetype to save the plots in if save_plots is set to True.
Options are: 'png', 'jpg', 'svg' &hellip;</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>Keys that specify data to be logged. The key that can be used to log data
in this function is 'moving_fish' (see extract_pulsefish()).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster labels, where deleted clusters have been set to -1.</dd>
<dt><strong><code>window</code></strong> :&ensp;<code>list</code> of <code>2 floats</code></dt>
<dd>Start and end of window selected for deleting moving fish in seconds.</dd>
<dt><strong><code>mf_dict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data to be logged is specified by return_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_moving_fish(clusters, eod_t, T, eod_heights, eod_widths, samplerate,
                       min_dt=0.25, stepsize=0.05, sliding_window_factor=2000,
                       verbose=0, plot_level=0, save_plot=False, save_path=&#39;&#39;,
                       ftype=&#39;pdf&#39;, return_data=[]):
    &#34;&#34;&#34;
    Use a sliding window to detect the minimum number of fish detected simultaneously, 
    then delete all other EOD clusters. 

    Do this only for EODs within the same width clusters, as a
    moving fish will preserve its EOD width.

    Parameters
    ----------
    clusters: list of ints
        EOD cluster labels.
    eod_t: list of floats
        Timepoints of the EODs (in seconds).
    T: float
        Length of recording (in seconds).
    eod_heights: list of floats
        EOD amplitudes.
    eod_widths: list of floats
        EOD widths (in seconds).
    samplerate: float
        Recording data samplerate.

    min_dt : float (optional)
        Minimum sliding window size (in seconds).
    stepsize : float (optional)
        Sliding window stepsize (in seconds).
    sliding_window_factor : float
        Multiplier for sliding window width,
        where the sliding window width = median(EOD_width)*sliding_window_factor.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plot : bool (optional)
        Set to True to save the plots created by plot_level.
    save_path : string (optional)
        Path to save data to. Only important if you wish to save data (save_data==True).
    ftype : string (optional)
        Define the filetype to save the plots in if save_plots is set to True.
        Options are: &#39;png&#39;, &#39;jpg&#39;, &#39;svg&#39; ...
    return_data : list of strings (optional)
        Keys that specify data to be logged. The key that can be used to log data
        in this function is &#39;moving_fish&#39; (see extract_pulsefish()).

    Returns
    -------
    clusters : list of ints
        Cluster labels, where deleted clusters have been set to -1.
    window : list of 2 floats
        Start and end of window selected for deleting moving fish in seconds.
    mf_dict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;
    mf_dict = {}
    
    if len(np.unique(clusters[clusters != -1])) == 0:
        return clusters, [0, 1], {}

    all_keep_clusters = []
    width_classes = merge_gaussians(eod_widths, np.copy(clusters), 0.75)   

    all_windows = []
    all_dts = []
    ev_num = 0
    for iw, w in enumerate(np.unique(width_classes[clusters &gt;= 0])):
        # initialize variables
        min_clusters = 100
        average_height = 0
        sparse_clusters = 100
        keep_clusters = []

        dt = max(min_dt, np.median(eod_widths[width_classes==w])*sliding_window_factor)
        window_start = 0
        window_end = dt

        wclusters = clusters[width_classes==w]
        weod_t = eod_t[width_classes==w]
        weod_heights = eod_heights[width_classes==w]
        weod_widths = eod_widths[width_classes==w]

        all_dts.append(dt)

        if verbose&gt;0:
            print(&#39;sliding window dt = %f&#39;%dt)

        # make W dependent on width??
        ignore_steps = np.zeros(len(np.arange(0, T-dt+stepsize, stepsize)))

        for i, t in enumerate(np.arange(0, T-dt+stepsize, stepsize)):
            current_clusters = wclusters[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]
            if len(np.unique(current_clusters))==0:
                ignore_steps[i-int(dt/stepsize):i+int(dt/stepsize)] = 1
                if verbose&gt;0:
                    print(&#39;No pulsefish in recording at T=%.2f:%.2f&#39; % (t, t+dt))

        
        x = np.arange(0, T-dt+stepsize, stepsize)
        y = np.ones(len(x))

        running_sum = np.ones(len(np.arange(0, T+stepsize, stepsize)))
        ulabs = np.unique(wclusters[wclusters&gt;=0])

        # sliding window
        for j, (t, ignore_step) in enumerate(zip(x, ignore_steps)):
            current_clusters = wclusters[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]
            current_widths = weod_widths[(weod_t&gt;=t)&amp;(weod_t&lt;t+dt)&amp;(wclusters!=-1)]

            unique_clusters = np.unique(current_clusters)
            y[j] = len(unique_clusters)

            if (len(unique_clusters) &lt;= min_clusters) and \
              (ignore_step==0) and \
              (len(unique_clusters !=1)):

                current_labels = np.isin(wclusters, unique_clusters)
                current_height = np.mean(weod_heights[current_labels])

                # compute nr of clusters that are too sparse
                clusters_after_deletion = np.unique(remove_sparse_detections(np.copy(clusters[np.isin(clusters, unique_clusters)]), samplerate*eod_widths[np.isin(clusters, unique_clusters)], samplerate, T))
                current_sparse_clusters = len(unique_clusters) - len(clusters_after_deletion[clusters_after_deletion!=-1])
               
                if current_sparse_clusters &lt;= sparse_clusters and \
                  ((current_sparse_clusters&lt;sparse_clusters) or
                   (current_height &gt; average_height) or
                   (len(unique_clusters) &lt; min_clusters)):
                    
                    keep_clusters = unique_clusters
                    min_clusters = len(unique_clusters)
                    average_height = current_height
                    window_end = t+dt
                    sparse_clusters = current_sparse_clusters

        all_keep_clusters.append(keep_clusters)
        all_windows.append(window_end)
        
        if &#39;moving_fish&#39; in return_data or plot_level&gt;0:
            if &#39;w&#39; in mf_dict:
                mf_dict[&#39;w&#39;].append(np.median(eod_widths[width_classes==w]))
                mf_dict[&#39;T&#39;] = T
                mf_dict[&#39;dt&#39;].append(dt)
                mf_dict[&#39;clusters&#39;].append(wclusters)
                mf_dict[&#39;t&#39;].append(weod_t)
                mf_dict[&#39;fishcount&#39;].append([x+0.5*(x[1]-x[0]), y])
                mf_dict[&#39;ignore_steps&#39;].append(ignore_steps)
            else:
                mf_dict[&#39;w&#39;] = [np.median(eod_widths[width_classes==w])]
                mf_dict[&#39;T&#39;] = [T]
                mf_dict[&#39;dt&#39;] = [dt]
                mf_dict[&#39;clusters&#39;] = [wclusters]
                mf_dict[&#39;t&#39;] = [weod_t]
                mf_dict[&#39;fishcount&#39;] = [[x+0.5*(x[1]-x[0]), y]]
                mf_dict[&#39;ignore_steps&#39;] = [ignore_steps]

    if verbose&gt;0:
        print(&#39;Estimated nr of pulsefish in recording: %i&#39;%len(all_keep_clusters))

    if plot_level&gt;0:
        plot_moving_fish(mf_dict[&#39;w&#39;], mf_dict[&#39;dt&#39;], mf_dict[&#39;clusters&#39;],mf_dict[&#39;t&#39;],
                         mf_dict[&#39;fishcount&#39;], T, mf_dict[&#39;ignore_steps&#39;])
        if save_plot:
            plt.savefig(&#39;%sdelete_moving_fish.%s&#39; % (save_path, ftype))
        # empty dict
        if &#39;moving_fish&#39; not in return_data:
            mf_dict = {}

    # delete all clusters that are not selected
    clusters[np.invert(np.isin(clusters, np.concatenate(all_keep_clusters)))] = -1

    return clusters, [np.max(all_windows)-np.max(all_dts), np.max(all_windows)], mf_dict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.delete_unreliable_fish"><code class="name flex">
<span>def <span class="ident">delete_unreliable_fish</span></span>(<span>clusters, eod_widths, eod_x, verbose=0, sdict={})</span>
</code></dt>
<dd>
<div class="desc"><p>Create a mask for EOD clusters that are either mixed with noise or other fish, or wavefish.</p>
<p>This is the case when the ration between the EOD width and the ISI is too large.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster labels.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>floats</code> or <code>ints</code></dt>
<dd>EOD widths in samples or seconds.</dd>
<dt><strong><code>eod_x</code></strong> :&ensp;<code>list</code> of <code>ints</code> or <code>floats</code></dt>
<dd>EOD times in samples or seconds.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>sdict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Dictionary that is used to log data. This is only used if a dictionary
was created by remove_artefacts().
For logging data in noise and wavefish discarding steps,
see remove_artefacts().</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mask</code></strong> :&ensp;<code>numpy array</code> of <code>booleans</code></dt>
<dd>Set to True for every unreliable EOD.</dd>
<dt><strong><code>sdict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data is only logged if a dictionary
was instantiated by remove_artefacts().</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_unreliable_fish(clusters, eod_widths, eod_x, verbose=0, sdict={}):
    &#34;&#34;&#34; Create a mask for EOD clusters that are either mixed with noise or other fish, or wavefish.
    
    This is the case when the ration between the EOD width and the ISI is too large.

    Parameters
    ----------
    clusters : list of ints
        Cluster labels.
    eod_widths : list of floats or ints
        EOD widths in samples or seconds.
    eod_x : list of ints or floats
        EOD times in samples or seconds.

    verbose : int (optional)
        Verbosity level.
    sdict : dictionary
        Dictionary that is used to log data. This is only used if a dictionary
        was created by remove_artefacts().
        For logging data in noise and wavefish discarding steps,
        see remove_artefacts().

    Returns
    -------
    mask : numpy array of booleans
        Set to True for every unreliable EOD.
    sdict : dictionary
        Key value pairs of logged data. Data is only logged if a dictionary
        was instantiated by remove_artefacts().
    &#34;&#34;&#34;
    mask = np.zeros(clusters.shape, dtype=bool)
    for cluster in np.unique(clusters[clusters &gt;= 0]):
        if len(eod_x[cluster == clusters]) &lt; 2:
            mask[clusters == cluster] = True
            if verbose&gt;0:
                print(&#39;deleting unreliable cluster %i, number of EOD times %d &lt; 2&#39; % (cluster, len(eod_x[cluster==clusters])))
        elif np.max(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters])) &gt; 0.5:
            if verbose&gt;0:
                print(&#39;deleting unreliable cluster %i, score=%f&#39; % (cluster, np.max(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters]))))
            mask[clusters==cluster] = True
        if &#39;vals_%d&#39; % cluster in sdict:
            sdict[&#39;vals_%d&#39; % cluster].append(np.median(eod_widths[clusters==cluster])/np.diff(eod_x[cluster==clusters]))
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask[clusters==cluster]))
    return mask, sdict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.delete_wavefish_and_sidepeaks"><code class="name flex">
<span>def <span class="ident">delete_wavefish_and_sidepeaks</span></span>(<span>data, clusters, eod_x, eod_widths, width_fac, max_slope_deviation=0.5, max_phases=4, verbose=0, sdict={})</span>
</code></dt>
<dd>
<div class="desc"><p>Create a mask for EODs that are likely from wavefish, or sidepeaks of bigger EODs.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>Raw recording data.</dd>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster labels.</dd>
<dt><strong><code>eod_x</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Indices of EOD times.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD widths in samples.</dd>
<dt><strong><code>width_fac</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplier for EOD analysis width.</dd>
<dt><strong><code>max_slope_deviation</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Maximum deviation of position of maximum slope in snippets from
center position in multiples of mean width of EOD.</dd>
<dt><strong><code>max_phases</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Maximum number of phases for any EOD.
If the mean EOD has more phases than this, it is not a pulse EOD.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional) </code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>sdict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Dictionary that is used to log data. This is only used if a dictionary
was created by remove_artefacts().
For logging data in noise and wavefish discarding steps, see remove_artefacts().</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mask_wave</code></strong> :&ensp;<code>numpy array</code> of <code>booleans</code></dt>
<dd>Set to True for every EOD which is a wavefish EOD.</dd>
<dt><strong><code>mask_sidepeak</code></strong> :&ensp;<code>numpy array</code> of <code>booleans</code></dt>
<dd>Set to True for every snippet which is centered around a sidepeak of an EOD.</dd>
<dt><strong><code>sdict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data is only logged if a dictionary
was instantiated by remove_artefacts().</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_wavefish_and_sidepeaks(data, clusters, eod_x, eod_widths,
                                  width_fac, max_slope_deviation=0.5,
                                  max_phases=4, verbose=0, sdict={}):
    &#34;&#34;&#34; Create a mask for EODs that are likely from wavefish, or sidepeaks of bigger EODs.

    Parameters
    ----------
    data : list of floats
        Raw recording data.
    clusters : list of ints
        Cluster labels.
    eod_x : list of ints
        Indices of EOD times.
    eod_widths : list of ints
        EOD widths in samples.
    width_fac : float
        Multiplier for EOD analysis width.

    max_slope_deviation: float (optional)
        Maximum deviation of position of maximum slope in snippets from
        center position in multiples of mean width of EOD.
    max_phases : int (optional)
        Maximum number of phases for any EOD. 
        If the mean EOD has more phases than this, it is not a pulse EOD.
    verbose : int (optional) 
        Verbosity level.
    sdict : dictionary
        Dictionary that is used to log data. This is only used if a dictionary
        was created by remove_artefacts().
        For logging data in noise and wavefish discarding steps, see remove_artefacts().

    Returns
    -------
    mask_wave: numpy array of booleans
        Set to True for every EOD which is a wavefish EOD.
    mask_sidepeak: numpy array of booleans
        Set to True for every snippet which is centered around a sidepeak of an EOD.
    sdict : dictionary
        Key value pairs of logged data. Data is only logged if a dictionary
        was instantiated by remove_artefacts().
    &#34;&#34;&#34;
    mask_wave = np.zeros(clusters.shape, dtype=bool)
    mask_sidepeak = np.zeros(clusters.shape, dtype=bool)

    for i, cluster in enumerate(np.unique(clusters[clusters &gt;= 0])):
        mean_width = np.mean(eod_widths[clusters == cluster])
        cutwidth = mean_width*width_fac
        current_x = eod_x[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
        current_clusters = clusters[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
        snippets = np.vstack([data[int(x-cutwidth):int(x+cutwidth)]
                              for x in current_x[current_clusters==cluster]])
        
        # extract information on main peaks and troughs:
        mean_eod = np.mean(snippets, axis=0)
        mean_eod = mean_eod - np.mean(mean_eod)

        # detect peaks and troughs on data + some maxima/minima at the
        # end, so that the sides are also considered for peak detection:
        pk, tr = detect_peaks(np.concatenate(([-10*mean_eod[0]], mean_eod, [10*mean_eod[-1]])),
                              np.std(mean_eod))
        pk = pk[(pk&gt;0)&amp;(pk&lt;len(mean_eod))]
        tr = tr[(tr&gt;0)&amp;(tr&lt;len(mean_eod))]

        if len(pk)&gt;0 and len(tr)&gt;0:
            idxs = np.sort(np.concatenate((pk, tr)))
            slopes = np.abs(np.diff(mean_eod[idxs]))
            m_slope = np.argmax(slopes)
            centered = np.min(np.abs(idxs[m_slope:m_slope+2] - len(mean_eod)//2))
            
            # compute all height differences of peaks and troughs within snippets.
            # if they are all similar, it is probably noise or a wavefish.
            idxs = np.sort(np.concatenate((pk, tr)))
            hdiffs = np.diff(mean_eod[idxs])

            if centered &gt; max_slope_deviation*mean_width:  # TODO: check, factor was probably 0.16
                if verbose &gt; 0:
                    print(&#39;Deleting cluster %i, which is a sidepeak&#39; % cluster)
                mask_sidepeak[clusters==cluster] = True

            w_diff = np.abs(np.diff(np.sort(np.concatenate((pk, tr)))))

            if np.abs(np.diff(idxs[m_slope:m_slope+2])) &lt; np.mean(eod_widths[clusters==cluster])*0.5 or len(pk) + len(tr)&gt;max_phases or np.min(w_diff)&gt;2*cutwidth/width_fac: #or len(hdiffs[np.abs(hdiffs)&gt;0.5*(np.max(mean_eod)-np.min(mean_eod))])&gt;max_phases:
                if verbose&gt;0:
                    print(&#39;Deleting cluster %i, which is a wavefish&#39; % cluster)
                mask_wave[clusters==cluster] = True
        if &#39;vals_%d&#39; % cluster in sdict:
            sdict[&#39;vals_%d&#39; % cluster].append([mean_eod, [pk, tr],
                                               idxs[m_slope:m_slope+2]])
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask_wave[clusters==cluster]))
            sdict[&#39;mask_%d&#39; % cluster].append(any(mask_sidepeak[clusters==cluster]))

    return mask_wave, mask_sidepeak, sdict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.detect_pulse_candidates"><code class="name flex">
<span>def <span class="ident">detect_pulse_candidates</span></span>(<span>data, samplerate, width_factor, min_peakwidth=5e-05, max_peakwidth=0.01, verbose=0, return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Detect potential pulse EODs in data.</p>
<p>Was <code>def extract_eod_times(data, samplerate, width_factor, interp_freq=500000,
max_peakwidth=0.01, min_peakwidth=None, verbose=0,
return_data=[], save_path='')</code> before.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>1-D array</code> of <code>float</code></dt>
<dd>The data to be analysed.</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Sampling rate of the data</dd>
<dt><strong><code>width_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>Factor for extracting EOD shapes.
Only EODs are extracted that can fully be analysed with this width.</dd>
<dt><strong><code>min_peakwidth</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Minimum width for peak detection in seconds.</dd>
<dt><strong><code>max_peakwidth</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Maximum width for peak detection in seconds.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>Keys that specify data to be logged. If 'peak_detection' is in <code>return_data</code>,
data of this function is logged (see extract_pulsefish()).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>x_peak</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Indices of EOD peaks in data.</dd>
<dt><strong><code>x_trough</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Indices of EOD troughs in data. There is one x_trough for each x_peak.</dd>
<dt><strong><code>eod_heights</code></strong> :&ensp;<code>array</code> of <code>floats</code></dt>
<dd>EOD heights for each x_peak.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>EOD widths for each x_peak (in samples).</dd>
<dt><strong><code>peak_detection_result</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data to be logged is specified by return_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_pulse_candidates(data, samplerate, width_factor,
                            min_peakwidth=0.00005, max_peakwidth=0.01,
                            verbose=0, return_data=[]):
    &#34;&#34;&#34; Detect potential pulse EODs in data.

    Was `def extract_eod_times(data, samplerate, width_factor, interp_freq=500000,
                      max_peakwidth=0.01, min_peakwidth=None, verbose=0,
                      return_data=[], save_path=&#39;&#39;)` before.

    Parameters
    ----------
    data: 1-D array of float
        The data to be analysed.
    samplerate: float
        Sampling rate of the data
    width_factor: float
        Factor for extracting EOD shapes.
        Only EODs are extracted that can fully be analysed with this width.

    min_peakwidth: float (optional)
        Minimum width for peak detection in seconds.
    max_peakwidth: float (optional)
        Maximum width for peak detection in seconds.

    verbose : int (optional)
        Verbosity level.
    return_data : list of strings (optional)
        Keys that specify data to be logged. If &#39;peak_detection&#39; is in `return_data`, 
        data of this function is logged (see extract_pulsefish()).

    Returns
    -------
    x_peak: array of ints
        Indices of EOD peaks in data.
    x_trough: array of ints
        Indices of EOD troughs in data. There is one x_trough for each x_peak.
    eod_heights: array of floats
        EOD heights for each x_peak.
    eod_widths: array of ints
        EOD widths for each x_peak (in samples).
    peak_detection_result : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.

    &#34;&#34;&#34;
    peak_detection_result = {}
    
    # standard deviation of data in small snippets:
    threshold = median_std_threshold(data, samplerate)  # TODO make this a parameter

    # detect peaks and troughs in the data:
    peak_indices, trough_indices = detect_peaks(data, threshold)
    if verbose &gt; 0:
        print(&#39;Peaks/troughs detected in data:                      %5d %5d&#39;
              % (len(peak_indices), len(trough_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_1=np.array(peak_indices),
                                     troughs_1=np.array(trough_indices))
    if len(peak_indices)&lt;2 or \
       len(trough_indices)&lt;2 or \
       len(peak_indices) &gt; len(data)/20:
        if verbose &gt; 0:
            print(&#39;No or too many peaks/troughs detected in data.&#39;)
        return np.array([]), np.array([]), np.array([]), np.array([]), \
            peak_detection_result

    # assign troughs to peaks:
    peak_indices, trough_indices, heights, widths = \
      assign_side_peaks(data, peak_indices, trough_indices, 0.25)
    # TODO: make 0.25 a parameter
    if verbose &gt; 1:
        print(&#39;Number of peaks after assigning side-peaks:          %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_2=np.array(peak_indices),
                                     troughs_2=np.array(trough_indices))

    # check widths:
    keep_events = ((widths&gt;min_peakwidth*samplerate) &amp;
                   (widths&lt;max_peakwidth*samplerate))
    peak_indices = peak_indices[keep_events]
    trough_indices = trough_indices[keep_events]
    heights = heights[keep_events]
    widths = widths[keep_events]
    if verbose &gt; 1:
        print(&#39;Number of peaks after checking pulse width:          %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_3=np.array(peak_indices),
                                     troughs_3=np.array(trough_indices))

    &#34;&#34;&#34;&#34;
    # merge connected peaks:
    peak_indices, trough_indices, heights, widths = \
      discard_connecting_eods(peak_indices, trough_indices, heights, widths)
    if verbose &gt; 1:
        print(&#39;Number of peaks after merging pulses:                %5d&#39;
              % (len(peak_indices)))
    if &#39;peak_detection&#39; in return_data:
        peak_detection_result.update(peaks_4=np.array(peak_indices),
                                     troughs_4=np.array(trough_indices))
    if len(peak_indices) == 0:
        if verbose &gt; 0:
            print(&#39;No peaks remain as pulse candidates.&#39;)
        return np.array([]), np.array([]), np.array([]), np.array([]), \
            peak_detection_result
    &#34;&#34;&#34;
    
    # only take those where the maximum cutwidth does not cause issues -
    # if the width_factor times the width + x is more than length.
    max_width = np.max(widths)*width_factor
    cut_idx = ((peak_indices - max_width &gt; 0) &amp;
               (peak_indices + max_width &lt; len(data)) &amp;
               (trough_indices - max_width &gt; 0) &amp;
               (trough_indices + max_width &lt; len(data)))

    if verbose &gt; 0:
        print(&#39;Remaining peaks after EOD extraction:                %5d&#39;
              % (p.sum(cut_idx)))
        print(&#39;&#39;)

    return peak_indices[cut_idx], trough_indices[cut_idx], heights[cut_idx], widths[cut_idx], peak_detection_result</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.discard_connecting_eods"><code class="name flex">
<span>def <span class="ident">discard_connecting_eods</span></span>(<span>x_peak, x_trough, heights, widths)</span>
</code></dt>
<dd>
<div class="desc"><p>If two detected EODs share the same closest trough, keep only the
highest peak.</p>
<pre><code>Parameters
----------
x_peak: list of ints
    Indices of EOD peaks.
x_trough: list of ints
    Indices of EOD troughs.
heights: list of floats
    EOD heights.
widths: list of ints
    EOD widths.

Returns
-------
x_peak, x_trough, heights, widths : lists of ints and floats
    EOD location and features of the non-discarded EODs
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@jit(nopython=True)
def discard_connecting_eods(x_peak, x_trough, heights, widths):
    &#34;&#34;&#34;If two detected EODs share the same closest trough, keep only the
highest peak.

    Parameters
    ----------
    x_peak: list of ints
        Indices of EOD peaks.
    x_trough: list of ints
        Indices of EOD troughs.
    heights: list of floats
        EOD heights.
    widths: list of ints
        EOD widths.

    Returns
    -------
    x_peak, x_trough, heights, widths : lists of ints and floats
        EOD location and features of the non-discarded EODs
    &#34;&#34;&#34;
    keep_idxs = np.ones(len(x_peak))

    for tr in np.unique(x_trough):
        if len(x_trough[x_trough==tr]) &gt; 1:
            slopes = heights[x_trough==tr]/widths[x_trough==tr]

            if (np.max(slopes)!=np.min(slopes)) and \
              (np.abs(np.max(slopes)-np.min(slopes))/(0.5*np.max(slopes)+0.5*np.min(slopes)) &gt; 0.25):
                keep_idxs[np.where(x_trough==tr)[0][np.argmin(heights[x_trough==tr]/widths[x_trough==tr])]] = 0
            else:
                keep_idxs[np.where(x_trough==tr)[0][np.argmin(heights[x_trough==tr])]] = 0
            
    return x_peak[np.where(keep_idxs==1)[0]], x_trough[np.where(keep_idxs==1)[0]], heights[np.where(keep_idxs==1)[0]], widths[np.where(keep_idxs==1)[0]]</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.extract_means"><code class="name flex">
<span>def <span class="ident">extract_means</span></span>(<span>data, eod_x, eod_peak_x, eod_tr_x, eod_widths, clusters, samplerate, width_fac, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract mean EODs and EOD timepoints for each EOD cluster.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>list</code> of <code>floats</code></dt>
<dd>Raw recording data.</dd>
<dt><strong><code>eod_x</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Locations of EODs in samples.</dd>
<dt><strong><code>eod_peak_x</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Locations of EOD peaks in samples.</dd>
<dt><strong><code>eod_tr_x</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Locations of EOD troughs in samples.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD widths in samples.</dd>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>samplerate of recording</dd>
<dt><strong><code>width_fac</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplication factor for window used to extract EOD.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mean_eods</code></strong> :&ensp;<code>list</code> of <code>2D arrays (3, eod_length)</code></dt>
<dd>The average EOD for each detected fish. First column is time in seconds,
second column the mean eod, third column the standard error.</dd>
<dt><strong><code>eod_times</code></strong> :&ensp;<code>list</code> of <code>1D arrays</code></dt>
<dd>For each detected fish the times of EOD in seconds.</dd>
<dt><strong><code>eod_peak_times</code></strong> :&ensp;<code>list</code> of <code>1D arrays</code></dt>
<dd>For each detected fish the times of EOD peaks in seconds.</dd>
<dt><strong><code>eod_trough_times</code></strong> :&ensp;<code>list</code> of <code>1D arrays</code></dt>
<dd>For each detected fish the times of EOD troughs in seconds.</dd>
<dt><strong><code>eod_labels</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster label for each detected fish.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_means(data, eod_x, eod_peak_x, eod_tr_x, eod_widths, clusters, samplerate,
                  width_fac, verbose=0):
    &#34;&#34;&#34; Extract mean EODs and EOD timepoints for each EOD cluster.

    Parameters
    ----------
    data: list of floats
        Raw recording data.
    eod_x: list of ints
        Locations of EODs in samples.
    eod_peak_x : list of ints
        Locations of EOD peaks in samples.
    eod_tr_x : list of ints
        Locations of EOD troughs in samples.
    eod_widths: list of ints
        EOD widths in samples.
    clusters: list of ints
        EOD cluster labels
    samplerate: float
        samplerate of recording  
    width_fac : float
        Multiplication factor for window used to extract EOD.
    
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    mean_eods: list of 2D arrays (3, eod_length)
        The average EOD for each detected fish. First column is time in seconds,
        second column the mean eod, third column the standard error.
    eod_times: list of 1D arrays
        For each detected fish the times of EOD in seconds.
    eod_peak_times: list of 1D arrays
        For each detected fish the times of EOD peaks in seconds.
    eod_trough_times: list of 1D arrays
        For each detected fish the times of EOD troughs in seconds.
    eod_labels: list of ints
        Cluster label for each detected fish.
    &#34;&#34;&#34;
    mean_eods, eod_times, eod_peak_times, eod_tr_times, eod_heights, cluster_labels = [], [], [], [], [], []

    for cluster in np.unique(clusters):
        if cluster!=-1:
            cutwidth = np.mean(eod_widths[clusters==cluster])*width_fac
            current_x = eod_x[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]
            current_clusters = clusters[(eod_x&gt;cutwidth) &amp; (eod_x&lt;(len(data)-cutwidth))]

            snippets = np.vstack([data[int(x-cutwidth):int(x+cutwidth)] for x in current_x[current_clusters==cluster]])
            mean_eod = np.mean(snippets, axis=0)
            eod_time = np.arange(len(mean_eod))/samplerate - cutwidth/samplerate

            mean_eod = np.vstack([eod_time, mean_eod, np.std(snippets, axis=0)])

            mean_eods.append(mean_eod)
            eod_times.append(eod_x[clusters==cluster]/samplerate)
            eod_heights.append(np.min(mean_eod)-np.max(mean_eod))
            eod_peak_times.append(eod_peak_x[clusters==cluster]/samplerate)
            eod_tr_times.append(eod_tr_x[clusters==cluster]/samplerate)
            cluster_labels.append(cluster)
           
    return [m for _, m in sorted(zip(eod_heights, mean_eods))], [t for _, t in sorted(zip(eod_heights, eod_times))], [pt for _, pt in sorted(zip(eod_heights, eod_peak_times))], [tt for _, tt in sorted(zip(eod_heights, eod_tr_times))], [c for _, c in sorted(zip(eod_heights, cluster_labels))]</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.extract_pulsefish"><code class="name flex">
<span>def <span class="ident">extract_pulsefish</span></span>(<span>data, samplerate, width_factor_shape=3, width_factor_wave=8, width_factor_display=4, verbose=0, plot_level=0, save_plots=False, save_path='', ftype='png', return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Extract and cluster pulse-type fish EODs from single channel data.</p>
<p>Takes recording data containing an unknown number of pulsefish and extracts the mean
EOD and EOD timepoints for each fish present in the recording.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>1-D array</code> of <code>float</code></dt>
<dd>The data to be analysed.</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Sampling rate of the data in Hertz.</dd>
<dt><strong><code>width_factor_shape</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Width multiplier used for EOD shape analysis.
EOD snippets are extracted based on width between the
peak and trough multiplied by the width factor.</dd>
<dt><strong><code>width_factor_wave</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Width multiplier used for wavefish detection.</dd>
<dt><strong><code>width_factor_display</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Width multiplier used for EOD mean extraction and display.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>plot_level</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Similar to verbosity levels, but with plots.
Only set to &gt; 0 for debugging purposes.</dd>
<dt><strong><code>save_plots</code></strong> :&ensp;<code>bool (optional)</code></dt>
<dd>Set to True to save the plots created by plot_level.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Path for saving plots.</dd>
<dt><strong><code>ftype</code></strong> :&ensp;<code>string (optional)</code></dt>
<dd>Define the filetype to save the plots in if save_plots is set to True.
Options are: 'png', 'jpg', 'svg' &hellip;</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>
<p>Specify data that should be logged and returned in a dictionary. Each clustering
step has a specific keyword that results in adding different variables to the log dictionary.
Optional keys for return_data and the resulting additional key-value pairs to the log dictionary are:</p>
<ul>
<li>
<p>'all_eod_times':</p>
<ul>
<li>'all_times':
list of two lists of floats.
All peak (<code>all_times[0]</code>) and trough times (<code>all_times[1]</code>) extracted
by the peak detection algorithm. Times are given in seconds.</li>
<li>'eod_troughtimes': list of 1D arrays.
The timepoints in seconds of each unique extracted EOD cluster,
where each 1D array encodes one cluster.</li>
</ul>
</li>
<li>
<p>'peak_detection':</p>
<ul>
<li>"data": 1D numpy array of floats.
Quadratically interpolated data which was used for peak detection.</li>
<li>"interp_fac": float.
Interpolation factor of raw data.</li>
<li>"peaks_1": 1D numpy array of ints.
Peak indices on interpolated data after first peak detection step.</li>
<li>"troughs_1": 1D numpy array of ints.
Peak indices on interpolated data after first peak detection step.</li>
<li>"peaks_2": 1D numpy array of ints.
Peak indices on interpolated data after second peak detection step.</li>
<li>"troughs_2": 1D numpy array of ints.
Peak indices on interpolated data after second peak detection step.</li>
<li>"peaks_3": 1D numpy array of ints.
Peak indices on interpolated data after third peak detection step.</li>
<li>"troughs_3": 1D numpy array of ints.
Peak indices on interpolated data after third peak detection step.</li>
<li>"peaks_4": 1D numpy array of ints.
Peak indices on interpolated data after fourth peak detection step.</li>
<li>"troughs_4": 1D numpy array of ints.
Peak indices on interpolated data after fourth peak detection step.</li>
</ul>
</li>
<li>
<p>'all_cluster_steps':</p>
<ul>
<li>'samplerate': float.
Samplerate of interpolated data.</li>
<li>'EOD_widths': list of three 1D numpy arrays.
The first list entry gives the unique labels of all width clusters
as a list of ints.
The second list entry gives the width values for each EOD in samples
as a 1D numpy array of ints.
The third list entry gives the width labels for each EOD
as a 1D numpy array of ints.</li>
<li>'EOD_heights': nested lists (2 layers) of three 1D numpy arrays.
The first list entry gives the unique labels of all height clusters
as a list of ints for each width cluster.
The second list entry gives the height values for each EOD
as a 1D numpy array of floats for each width cluster.
The third list entry gives the height labels for each EOD
as a 1D numpy array of ints for each width cluster.</li>
<li>'EOD_shapes': nested lists (3 layers) of three 1D numpy arrays
The first list entry gives the raw EOD snippets as a 2D numpy array
for each height cluster in a width cluster.
The second list entry gives the snippet PCA values for each EOD
as a 2D numpy array of floats for each height cluster in a width cluster.
The third list entry gives the shape labels for each EOD as a 1D numpy array
of ints for each height cluster in a width cluster.</li>
<li>'discarding_masks': Nested lists (two layers) of 1D numpy arrays.
The masks of EODs that are discarded by the discarding step of the algorithm.
The masks are 1D boolean arrays where instances that are set to True are
discarded by the algorithm. Discarding masks are saved in nested lists
that represent the width and height clusters.</li>
<li>'merge_masks': Nested lists (two layers) of 2D numpy arrays.
The masks of EODs that are discarded by the merging step of the algorithm.
The masks are 2D boolean arrays where for each sample point <code>i</code> either
<code>merge_mask[i,0]</code> or <code>merge_mask[i,1]</code> is set to True. Here, merge_mask[:,0]
represents the peak-centered clusters and <code>merge_mask[:,1]</code> represents the
trough-centered clusters. Merge masks are saved in nested lists that
represent the width and height clusters.</li>
</ul>
</li>
<li>
<p>'BGM_width':</p>
<ul>
<li>'BGM_width': dictionary<ul>
<li>'x': 1D numpy array of floats.
BGM input values (in this case the EOD widths),</li>
<li>'use_log': boolean.
True if the z-scored logarithm of the data was used as BGM input.</li>
<li>'BGM': list of three 1D numpy arrays.
The first instance are the weights of the Gaussian fits.
The second instance are the means of the Gaussian fits.
The third instance are the variances of the Gaussian fits.</li>
<li>'labels': 1D numpy array of ints.
Labels defined by BGM model (before merging based on merge factor).</li>
<li>xlab': string.
Label for plot (defines the units of the BGM data).</li>
</ul>
</li>
</ul>
</li>
<li>
<p>'BGM_height':
This key adds a new dictionary for each width cluster.</p>
<ul>
<li>'BGM_height_<em>n</em>' : dictionary, where <em>n</em> defines the width cluster as an int.<ul>
<li>'x': 1D numpy array of floats.
BGM input values (in this case the EOD heights),</li>
<li>'use_log': boolean.
True if the z-scored logarithm of the data was used as BGM input.</li>
<li>'BGM': list of three 1D numpy arrays.
The first instance are the weights of the Gaussian fits.
The second instance are the means of the Gaussian fits.
The third instance are the variances of the Gaussian fits.</li>
<li>'labels': 1D numpy array of ints.
Labels defined by BGM model (before merging based on merge factor).</li>
<li>'xlab': string.
Label for plot (defines the units of the BGM data).</li>
</ul>
</li>
</ul>
</li>
<li>
<p>'snippet_clusters':
This key adds a new dictionary for each height cluster.</p>
<ul>
<li>'snippet_clusters<em>_n_m_p</em>' : dictionary, where <em>n</em> defines the width cluster
(int), <em>m</em> defines the height cluster (int) and <em>p</em> defines shape clustering
on peak or trough centered EOD snippets (string: 'peak' or 'trough').<ul>
<li>'raw_snippets': 2D numpy array (nsamples, nfeatures).
Raw EOD snippets.</li>
<li>'snippets': 2D numpy array.
Normalized EOD snippets.</li>
<li>'features': 2D numpy array.(nsamples, nfeatures)
PCA values for each normalized EOD snippet.</li>
<li>'clusters': 1D numpy array of ints.
Cluster labels.</li>
<li>'samplerate': float.
Samplerate of snippets.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>'eod_deletion':
This key adds two dictionaries for each (peak centered) shape cluster,
where <em>cluster</em> (int) is the unique shape cluster label.</p>
<ul>
<li>'mask_<em>cluster</em>' : list of four booleans.
The mask for each cluster discarding step.
The first instance represents the artefact masks, where artefacts
are set to True.
The second instance represents the unreliable cluster masks,
where unreliable clusters are set to True.
The third instance represents the wavefish masks, where wavefish
are set to True.
The fourth instance represents the sidepeak masks, where sidepeaks
are set to True.</li>
<li>'vals_<em>cluster</em>' : list of lists.
All variables that are used for each cluster deletion step.
The first instance is a list of two 1D numpy arrays: the mean EOD and
the FFT of that mean EOD.
The second instance is a 1D numpy array with all EOD width to ISI ratios.
The third instance is a list with three entries:
The first entry is a 1D numpy array zoomed out version of the mean EOD.
The second entry is a list of two 1D numpy arrays that define the peak
and trough indices of the zoomed out mean EOD.
The third entry contains a list of two values that represent the
peak-trough pair in the zoomed out mean EOD with the largest height
difference.</li>
<li>'samplerate' : float.
EOD snippet samplerate.</li>
</ul>
</li>
<li>
<p>'masks': </p>
<ul>
<li>'masks' : 2D numpy array (4,N).
Each row contains masks for each EOD detected by the EOD peakdetection step.
The first row defines the artefact masks, the second row defines the
unreliable EOD masks,
the third row defines the wavefish masks and the fourth row defines
the sidepeak masks.</li>
</ul>
</li>
<li>
<p>'moving_fish':</p>
<ul>
<li>'moving_fish': dictionary.<ul>
<li>'w' : list of floats.
Median width for each width cluster that the moving fish algorithm is
computed on (in seconds).</li>
<li>'T' : list of floats.
Lenght of analyzed recording for each width cluster (in seconds).</li>
<li>'dt' : list of floats.
Sliding window size (in seconds) for each width cluster.</li>
<li>'clusters' : list of 1D numpy int arrays.
Cluster labels for each EOD cluster in a width cluster.</li>
<li>'t' : list of 1D numpy float arrays.
EOD emission times for each EOD in a width cluster.</li>
<li>'fishcount' : list of lists.
Sliding window timepoints and fishcounts for each width cluster.</li>
<li>'ignore_steps' : list of 1D int arrays.
Mask for fishcounts that were ignored (ignored if True) in the
moving_fish analysis.</li>
</ul>
</li>
</ul>
</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mean_eods</code></strong> :&ensp;<code>list</code> of <code>2D arrays (3, eod_length)</code></dt>
<dd>The average EOD for each detected fish. First column is time in seconds,
second column the mean eod, third column the standard error.</dd>
<dt><strong><code>eod_times</code></strong> :&ensp;<code>list</code> of <code>1D arrays</code></dt>
<dd>For each detected fish the times of EOD peaks or troughs in seconds.
Use these timepoints for EOD averaging.</dd>
<dt><strong><code>eod_peaktimes</code></strong> :&ensp;<code>list</code> of <code>1D arrays</code></dt>
<dd>For each detected fish the times of EOD peaks in seconds.</dd>
<dt><strong><code>zoom_window</code></strong> :&ensp;<code>tuple</code> of <code>floats</code></dt>
<dd>Start and endtime of suggested window for plotting EOD timepoints.</dd>
<dt><strong><code>log_dict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Dictionary with logged variables, where variables to log are specified
by <code>return_data</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_pulsefish(data, samplerate, width_factor_shape=3, width_factor_wave=8,
                      width_factor_display=4, verbose=0, plot_level=0, save_plots=False,
                      save_path=&#39;&#39;, ftype=&#39;png&#39;, return_data=[]):
    &#34;&#34;&#34; Extract and cluster pulse-type fish EODs from single channel data.
    
    Takes recording data containing an unknown number of pulsefish and extracts the mean 
    EOD and EOD timepoints for each fish present in the recording.
    
    Parameters
    ----------
    data: 1-D array of float
        The data to be analysed.
    samplerate: float
        Sampling rate of the data in Hertz.

    width_factor_shape : float (optional)
        Width multiplier used for EOD shape analysis.
        EOD snippets are extracted based on width between the 
        peak and trough multiplied by the width factor.
    width_factor_wave : float (optional)
        Width multiplier used for wavefish detection.
    width_factor_display : float (optional)
        Width multiplier used for EOD mean extraction and display.
    verbose : int (optional)
        Verbosity level.
    plot_level : int (optional)
        Similar to verbosity levels, but with plots. 
        Only set to &gt; 0 for debugging purposes.
    save_plots : bool (optional)
        Set to True to save the plots created by plot_level.
    save_path: string (optional)
        Path for saving plots.
    ftype : string (optional)
        Define the filetype to save the plots in if save_plots is set to True.
        Options are: &#39;png&#39;, &#39;jpg&#39;, &#39;svg&#39; ...
    return_data : list of strings (optional)
        Specify data that should be logged and returned in a dictionary. Each clustering 
        step has a specific keyword that results in adding different variables to the log dictionary.
        Optional keys for return_data and the resulting additional key-value pairs to the log dictionary are:
        
        - &#39;all_eod_times&#39;:
            - &#39;all_times&#39;:  list of two lists of floats.
                All peak (`all_times[0]`) and trough times (`all_times[1]`) extracted
                by the peak detection algorithm. Times are given in seconds.
            - &#39;eod_troughtimes&#39;: list of 1D arrays.
                The timepoints in seconds of each unique extracted EOD cluster,
                where each 1D array encodes one cluster.
        
        - &#39;peak_detection&#39;:
            - &#34;data&#34;: 1D numpy array of floats.
                Quadratically interpolated data which was used for peak detection.
            - &#34;interp_fac&#34;: float.
                Interpolation factor of raw data.
            - &#34;peaks_1&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after first peak detection step.
            - &#34;troughs_1&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after first peak detection step.
            - &#34;peaks_2&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after second peak detection step.
            - &#34;troughs_2&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after second peak detection step.
            - &#34;peaks_3&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after third peak detection step.
            - &#34;troughs_3&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after third peak detection step.
            - &#34;peaks_4&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after fourth peak detection step.
            - &#34;troughs_4&#34;: 1D numpy array of ints.
                Peak indices on interpolated data after fourth peak detection step.

        - &#39;all_cluster_steps&#39;:
            - &#39;samplerate&#39;: float.
                Samplerate of interpolated data.
            - &#39;EOD_widths&#39;: list of three 1D numpy arrays.
                The first list entry gives the unique labels of all width clusters
                as a list of ints.
                The second list entry gives the width values for each EOD in samples
                as a 1D numpy array of ints.
                The third list entry gives the width labels for each EOD
                as a 1D numpy array of ints.
            - &#39;EOD_heights&#39;: nested lists (2 layers) of three 1D numpy arrays.
                The first list entry gives the unique labels of all height clusters
                as a list of ints for each width cluster.
                The second list entry gives the height values for each EOD
                as a 1D numpy array of floats for each width cluster.
                The third list entry gives the height labels for each EOD
                as a 1D numpy array of ints for each width cluster.
            - &#39;EOD_shapes&#39;: nested lists (3 layers) of three 1D numpy arrays
                The first list entry gives the raw EOD snippets as a 2D numpy array
                for each height cluster in a width cluster.
                The second list entry gives the snippet PCA values for each EOD
                as a 2D numpy array of floats for each height cluster in a width cluster.
                The third list entry gives the shape labels for each EOD as a 1D numpy array
                of ints for each height cluster in a width cluster.
            - &#39;discarding_masks&#39;: Nested lists (two layers) of 1D numpy arrays.
                The masks of EODs that are discarded by the discarding step of the algorithm.
                The masks are 1D boolean arrays where instances that are set to True are
                discarded by the algorithm. Discarding masks are saved in nested lists
                that represent the width and height clusters.
            - &#39;merge_masks&#39;: Nested lists (two layers) of 2D numpy arrays.
                The masks of EODs that are discarded by the merging step of the algorithm.
                The masks are 2D boolean arrays where for each sample point `i` either
                `merge_mask[i,0]` or `merge_mask[i,1]` is set to True. Here, merge_mask[:,0]
                represents the peak-centered clusters and `merge_mask[:,1]` represents the
                trough-centered clusters. Merge masks are saved in nested lists that
                represent the width and height clusters.

        - &#39;BGM_width&#39;:
            - &#39;BGM_width&#39;: dictionary
                - &#39;x&#39;: 1D numpy array of floats.
                    BGM input values (in this case the EOD widths),
                - &#39;use_log&#39;: boolean.
                    True if the z-scored logarithm of the data was used as BGM input.
                - &#39;BGM&#39;: list of three 1D numpy arrays.
                    The first instance are the weights of the Gaussian fits.
                    The second instance are the means of the Gaussian fits.
                    The third instance are the variances of the Gaussian fits.
                - &#39;labels&#39;: 1D numpy array of ints.
                    Labels defined by BGM model (before merging based on merge factor).
                - xlab&#39;: string.
                    Label for plot (defines the units of the BGM data).

        - &#39;BGM_height&#39;:
            This key adds a new dictionary for each width cluster.
            - &#39;BGM_height_*n*&#39; : dictionary, where *n* defines the width cluster as an int.
                - &#39;x&#39;: 1D numpy array of floats.
                    BGM input values (in this case the EOD heights),
                - &#39;use_log&#39;: boolean.
                    True if the z-scored logarithm of the data was used as BGM input.
                - &#39;BGM&#39;: list of three 1D numpy arrays.
                    The first instance are the weights of the Gaussian fits.
                    The second instance are the means of the Gaussian fits.
                    The third instance are the variances of the Gaussian fits.
                - &#39;labels&#39;: 1D numpy array of ints.
                    Labels defined by BGM model (before merging based on merge factor).
                - &#39;xlab&#39;: string.
                    Label for plot (defines the units of the BGM data).

        - &#39;snippet_clusters&#39;:
            This key adds a new dictionary for each height cluster.
            - &#39;snippet_clusters*_n_m_p*&#39; : dictionary, where *n* defines the width cluster
              (int), *m* defines the height cluster (int) and *p* defines shape clustering
              on peak or trough centered EOD snippets (string: &#39;peak&#39; or &#39;trough&#39;).
                - &#39;raw_snippets&#39;: 2D numpy array (nsamples, nfeatures).
                    Raw EOD snippets.
                - &#39;snippets&#39;: 2D numpy array.
                    Normalized EOD snippets.
                - &#39;features&#39;: 2D numpy array.(nsamples, nfeatures)
                    PCA values for each normalized EOD snippet.
                - &#39;clusters&#39;: 1D numpy array of ints.
                    Cluster labels.
                - &#39;samplerate&#39;: float.
                    Samplerate of snippets.

        - &#39;eod_deletion&#39;:
            This key adds two dictionaries for each (peak centered) shape cluster,
            where *cluster* (int) is the unique shape cluster label.
            - &#39;mask_*cluster*&#39; : list of four booleans.
                The mask for each cluster discarding step. 
                The first instance represents the artefact masks, where artefacts
                are set to True.
                The second instance represents the unreliable cluster masks,
                where unreliable clusters are set to True.
                The third instance represents the wavefish masks, where wavefish
                are set to True.
                The fourth instance represents the sidepeak masks, where sidepeaks
                are set to True.
            - &#39;vals_*cluster*&#39; : list of lists.
                All variables that are used for each cluster deletion step.
                The first instance is a list of two 1D numpy arrays: the mean EOD and
                the FFT of that mean EOD.
                The second instance is a 1D numpy array with all EOD width to ISI ratios.
                The third instance is a list with three entries: 
                    The first entry is a 1D numpy array zoomed out version of the mean EOD.
                    The second entry is a list of two 1D numpy arrays that define the peak
                    and trough indices of the zoomed out mean EOD.
                    The third entry contains a list of two values that represent the
                    peak-trough pair in the zoomed out mean EOD with the largest height
                    difference.
            - &#39;samplerate&#39; : float.
                EOD snippet samplerate.

        - &#39;masks&#39;: 
            - &#39;masks&#39; : 2D numpy array (4,N).
                Each row contains masks for each EOD detected by the EOD peakdetection step. 
                The first row defines the artefact masks, the second row defines the
                unreliable EOD masks, 
                the third row defines the wavefish masks and the fourth row defines
                the sidepeak masks.

        - &#39;moving_fish&#39;:
            - &#39;moving_fish&#39;: dictionary.
                - &#39;w&#39; : list of floats.
                    Median width for each width cluster that the moving fish algorithm is
                    computed on (in seconds).
                - &#39;T&#39; : list of floats.
                    Lenght of analyzed recording for each width cluster (in seconds).
                - &#39;dt&#39; : list of floats.
                    Sliding window size (in seconds) for each width cluster.
                - &#39;clusters&#39; : list of 1D numpy int arrays.
                    Cluster labels for each EOD cluster in a width cluster.
                - &#39;t&#39; : list of 1D numpy float arrays.
                    EOD emission times for each EOD in a width cluster.
                - &#39;fishcount&#39; : list of lists.
                    Sliding window timepoints and fishcounts for each width cluster.
                - &#39;ignore_steps&#39; : list of 1D int arrays.
                    Mask for fishcounts that were ignored (ignored if True) in the
                    moving_fish analysis.
        
    Returns
    -------
    mean_eods: list of 2D arrays (3, eod_length)
        The average EOD for each detected fish. First column is time in seconds,
        second column the mean eod, third column the standard error.
    eod_times: list of 1D arrays
        For each detected fish the times of EOD peaks or troughs in seconds.
        Use these timepoints for EOD averaging.
    eod_peaktimes: list of 1D arrays
        For each detected fish the times of EOD peaks in seconds.
    zoom_window: tuple of floats
        Start and endtime of suggested window for plotting EOD timepoints.
    log_dict: dictionary
        Dictionary with logged variables, where variables to log are specified
        by `return_data`.
    &#34;&#34;&#34;
    if verbose &gt; 0:
        print(&#39;&#39;)
        if verbose &gt; 1:
            print(70*&#39;#&#39;)
        print(&#39;##### extract_pulsefish&#39;, 46*&#39;#&#39;)

    if (save_plots and plot_level&gt;0 and save_path):
        # create folder to save things in.
        if not os.path.exists(save_path):
            os.makedirs(save_path)
    else:
        save_path = &#39;&#39;

    mean_eods, eod_times, eod_peaktimes, zoom_window = [], [], [], []
    log_dict = {}

    # interpolate:
    i_samplerate = 500000.0
    #i_samplerate = samplerate
    try:
        f = interp1d(np.arange(len(data))/samplerate, data, kind=&#39;quadratic&#39;)
        i_data = f(np.arange(0.0, (len(data)-1)/samplerate, 1.0/i_samplerate))
    except MemoryError:
        i_samplerate = samplerate
        i_data = data
    log_dict[&#39;data&#39;] = i_data               # TODO: could be removed
    log_dict[&#39;samplerate&#39;] = i_samplerate   # TODO: could be removed
    log_dict[&#39;i_data&#39;] = i_data
    log_dict[&#39;i_samplerate&#39;] = i_samplerate
    # log_dict[&#34;interp_fac&#34;] = interp_fac  # TODO: is not set anymore
                                         
    
    # extract peaks:
    x_peak, x_trough, eod_heights, eod_widths, pd_log_dict = \
      detect_pulse_candidates(i_data, i_samplerate,
                              np.max([width_factor_shape, width_factor_display, width_factor_wave]),
                              verbose=verbose-1, return_data=return_data)
    
    if len(x_peak) &gt; 0:
        # cluster
        clusters, x_merge, c_log_dict = cluster(x_peak, x_trough, eod_heights, eod_widths,
                                                i_data, i_samplerate,
                                                width_factor_shape, width_factor_wave,
                                                verbose=verbose-1, plot_level=plot_level-1,
                                                save_plots=save_plots, save_path=save_path,
                                                ftype=ftype, return_data=return_data) 

        # extract mean eods and times
        mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels = \
          extract_means(i_data, x_merge, x_peak, x_trough, eod_widths, clusters,
                        i_samplerate, width_factor_display, verbose=verbose-1)

        # determine clipped clusters (save them, but ignore in other steps)
        clusters, clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes = \
          find_clipped_clusters(clusters, mean_eods, eod_times, eod_peaktimes,
                                eod_troughtimes, cluster_labels, width_factor_display,
                                verbose=verbose-1)

        # delete the moving fish
        clusters, zoom_window, mf_log_dict = \
          delete_moving_fish(clusters, x_merge/i_samplerate, len(data)/samplerate,
                             eod_heights, eod_widths/i_samplerate, i_samplerate,
                             verbose=verbose-1, plot_level=plot_level-1, save_plot=save_plots,
                             save_path=save_path, ftype=ftype, return_data=return_data)
        
        if &#39;moving_fish&#39; in return_data:
            log_dict[&#39;moving_fish&#39;] = mf_log_dict

        clusters = remove_sparse_detections(clusters, eod_widths, i_samplerate,
                                            len(data)/samplerate, verbose=verbose-1)

        # extract mean eods
        mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels = \
          extract_means(i_data, x_merge, x_peak, x_trough, eod_widths,
                        clusters, i_samplerate, width_factor_display, verbose=verbose-1)

        mean_eods.extend(clipped_eods)
        eod_times.extend(clipped_times)
        eod_peaktimes.extend(clipped_peaktimes)
        eod_troughtimes.extend(clipped_troughtimes)

        if plot_level &gt; 0:
            plot_all(data, eod_peaktimes, eod_troughtimes, samplerate, mean_eods)
            if save_plots:
                plt.savefig(&#39;%sextract_pulsefish_results.%s&#39; % (save_path, ftype))
        if save_plots:
            plt.close(&#39;all&#39;)
    
        if &#39;all_eod_times&#39; in return_data:
            log_dict[&#39;all_times&#39;] = [x_peak/i_samplerate, x_trough/i_samplerate]
            log_dict[&#39;eod_troughtimes&#39;] = eod_troughtimes
        
        log_dict.update(pd_log_dict)
        log_dict.update(c_log_dict)

    return mean_eods, eod_times, eod_peaktimes, zoom_window, log_dict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.extract_snippet_features"><code class="name flex">
<span>def <span class="ident">extract_snippet_features</span></span>(<span>data, eod_x, eod_widths, eod_heights, width_factor, n_pc=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract snippets from recording data, normalize them, and perform PCA.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>1D numpy array</code> of <code>floats</code></dt>
<dd>Recording data.</dd>
<dt><strong><code>eod_x</code></strong> :&ensp;<code>1D array</code> of <code>ints</code></dt>
<dd>Locations of EODs as indices.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>1D array</code> of <code>ints</code></dt>
<dd>EOD widths in samples.</dd>
<dt><strong><code>eod_heights</code></strong> :&ensp;<code>1D array</code> of <code>floats</code></dt>
<dd>EOD heights.</dd>
<dt><strong><code>width_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>Multiplier for extracting EOD snippets</dd>
<dt><strong><code>n_pc</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Number of PCs to use for PCA.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>raw_snippets</code></strong> :&ensp;<code>2D numpy array (N, EOD_width)</code></dt>
<dd>Raw extracted EOD snippets.</dd>
<dt><strong><code>snippets</code></strong> :&ensp;<code>2D numpy array (N, EOD_width)</code></dt>
<dd>Normalized EOD snippets</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>2D numpy array (N,n_pc)</code></dt>
<dd>PC values of EOD snippets</dd>
<dt><strong><code>bg_ratio</code></strong> :&ensp;<code>1D numpy array (N)</code></dt>
<dd>Ratio of the background activity slopes compared to EOD height.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_snippet_features(data, eod_x, eod_widths, eod_heights, width_factor, n_pc=5):
    &#34;&#34;&#34; Extract snippets from recording data, normalize them, and perform PCA.

    Parameters
    ----------
    data : 1D numpy array of floats
        Recording data.
    eod_x : 1D array of ints
        Locations of EODs as indices.
    eod_widths : 1D array of ints
        EOD widths in samples.
    eod_heights: 1D array of floats
        EOD heights.
    width_factor: float
        Multiplier for extracting EOD snippets        

    n_pc : int (optional)
        Number of PCs to use for PCA.

    Returns
    -------
    raw_snippets : 2D numpy array (N, EOD_width)
        Raw extracted EOD snippets.
    snippets : 2D numpy array (N, EOD_width)
        Normalized EOD snippets
    features : 2D numpy array (N,n_pc)
        PC values of EOD snippets
    bg_ratio : 1D numpy array (N)
        Ratio of the background activity slopes compared to EOD height.
    &#34;&#34;&#34;

    # extract snippets with corresponding width
    width = width_factor*np.median(eod_widths)
    raw_snippets = np.vstack([data[int(x-width):int(x+width)] for x in eod_x])

    # subtract the slope and normalize the snippets
    snippets, bg_ratio = subtract_slope(np.copy(raw_snippets), eod_heights)
    snippets = StandardScaler().fit_transform(snippets.T).T

    # scale so that the absolute integral = 1.
    snippets = (snippets.T/np.sum(np.abs(snippets), axis=1)).T

    # compute features for clustering on waveform
    features = PCA(n_pc).fit(snippets).transform(snippets)

    return raw_snippets, snippets, features, bg_ratio</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.find_clipped_clusters"><code class="name flex">
<span>def <span class="ident">find_clipped_clusters</span></span>(<span>clusters, mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels, width_factor, clip_threshold=0.9, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Detect EODs that are clipped and set all clusterlabels of these clipped EODs to -1.</p>
<p>Also return the mean EODs and timepoints of these clipped EODs.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Cluster labels for each EOD in a recording.</dd>
<dt><strong><code>mean_eods</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>Mean EOD waveform for each cluster.</dd>
<dt><strong><code>eod_times</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>EOD timepoints for each EOD cluster.</dd>
<dt><strong><code>eod_peaktimes</code></strong></dt>
<dd>EOD peaktimes for each EOD cluster.</dd>
<dt><strong><code>eod_troughtimes</code></strong></dt>
<dd>EOD troughtimes for each EOD cluster.</dd>
<dt><strong><code>cluster_labels</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Unique EOD clusterlabels.</dd>
<dt><strong><code>clip_threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Threshold for detecting clipped EODs.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code></dt>
<dd>Verbosity level.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>array</code> of <code>ints</code></dt>
<dd>Cluster labels for each EOD in the recording, where clipped EODs have been set to -1.</dd>
<dt><strong><code>clipped_eods</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>Mean EOD waveforms for each clipped EOD cluster.</dd>
<dt><strong><code>clipped_times</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>EOD timepoints for each clipped EOD cluster.</dd>
<dt><strong><code>clipped_peaktimes</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>EOD peaktimes for each clipped EOD cluster.</dd>
<dt><strong><code>clipped_troughtimes</code></strong> :&ensp;<code>list</code> of <code>numpy arrays</code></dt>
<dd>EOD troughtimes for each clipped EOD cluster.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_clipped_clusters(clusters, mean_eods, eod_times, eod_peaktimes, eod_troughtimes,
                          cluster_labels, width_factor, clip_threshold=0.9, verbose=0):
    &#34;&#34;&#34; Detect EODs that are clipped and set all clusterlabels of these clipped EODs to -1.
                          
    Also return the mean EODs and timepoints of these clipped EODs.

    Parameters
    ----------
    clusters: array of ints
        Cluster labels for each EOD in a recording.
    mean_eods: list of numpy arrays
        Mean EOD waveform for each cluster.
    eod_times: list of numpy arrays
        EOD timepoints for each EOD cluster.
    eod_peaktimes
        EOD peaktimes for each EOD cluster.
    eod_troughtimes
        EOD troughtimes for each EOD cluster.
    cluster_labels: numpy array
        Unique EOD clusterlabels.
    clip_threshold: float
        Threshold for detecting clipped EODs.
    
    verbose: int
        Verbosity level.

    Returns
    -------
    clusters : array of ints
        Cluster labels for each EOD in the recording, where clipped EODs have been set to -1.
    clipped_eods : list of numpy arrays
        Mean EOD waveforms for each clipped EOD cluster.
    clipped_times : list of numpy arrays
        EOD timepoints for each clipped EOD cluster.
    clipped_peaktimes : list of numpy arrays
        EOD peaktimes for each clipped EOD cluster.
    clipped_troughtimes : list of numpy arrays
        EOD troughtimes for each clipped EOD cluster.
    &#34;&#34;&#34;
    clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes, clipped_labels = [], [], [], [], []

    for mean_eod, eod_time, eod_peaktime, eod_troughtime,label in zip(mean_eods, eod_times, eod_peaktimes, eod_troughtimes, cluster_labels):
        
        if (np.count_nonzero(mean_eod[1]&gt;clip_threshold) &gt; len(mean_eod[1])/(width_factor*2)) or (np.count_nonzero(mean_eod[1] &lt; -clip_threshold) &gt; len(mean_eod[1])/(width_factor*2)):
            clipped_eods.append(mean_eod)
            clipped_times.append(eod_time)
            clipped_peaktimes.append(eod_peaktime)
            clipped_troughtimes.append(eod_troughtime)
            clipped_labels.append(label)
            if verbose&gt;0:
                print(&#39;clipped pulsefish&#39;)

    clusters[np.isin(clusters, clipped_labels)] = -1

    return clusters, clipped_eods, clipped_times, clipped_peaktimes, clipped_troughtimes</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.jit"><code class="name flex">
<span>def <span class="ident">jit</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit(*args, **kwargs):
    def decorator_jit(func):
        return func
    return decorator_jit</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.merge_clusters"><code class="name flex">
<span>def <span class="ident">merge_clusters</span></span>(<span>clusters_1, clusters_2, x_1, x_2, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge clusters resulting from two clustering methods.</p>
<p>This method only works
if clustering is performed on the same EODs
with the same ordering, where there
is a one to one mapping from
clusters_1 to clusters_2. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters_1</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels for cluster method 1.</dd>
<dt><strong><code>clusters_2</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels for cluster method 2.</dd>
<dt><strong><code>x_1</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Indices of EODs for cluster method 1 (clusters_1).</dd>
<dt><strong><code>x_2</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Indices of EODs for cluster method 2 (clusters_2).</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Merged clusters.</dd>
<dt><strong><code>x_merged</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Merged cluster indices.</dd>
<dt><strong><code>mask</code></strong> :&ensp;<code>2d numpy array</code> of <code>ints (N, 2)</code></dt>
<dd>Mask for clusters that are selected from clusters_1 (mask[:,0]) and
from clusters_2 (mask[:,1]).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_clusters(clusters_1, clusters_2, x_1, x_2, verbose=0): 
    &#34;&#34;&#34; Merge clusters resulting from two clustering methods.

    This method only works  if clustering is performed on the same EODs
    with the same ordering, where there  is a one to one mapping from
    clusters_1 to clusters_2. 

    Parameters
    ----------
    clusters_1: list of ints
        EOD cluster labels for cluster method 1.
    clusters_2: list of ints
        EOD cluster labels for cluster method 2.
    x_1: list of ints
        Indices of EODs for cluster method 1 (clusters_1).
    x_2: list of ints
        Indices of EODs for cluster method 2 (clusters_2).
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    clusters : list of ints
        Merged clusters.
    x_merged : list of ints
        Merged cluster indices.
    mask : 2d numpy array of ints (N, 2)
        Mask for clusters that are selected from clusters_1 (mask[:,0]) and
        from clusters_2 (mask[:,1]).
    &#34;&#34;&#34;
    if verbose &gt; 0:
        print(&#39;\nMerge cluster:&#39;)

    # these arrays become 1 for each EOD that is chosen from that array
    c1_keep = np.zeros(len(clusters_1))
    c2_keep = np.zeros(len(clusters_2))

    # add n to one of the cluster lists to avoid overlap
    ovl = np.max(clusters_1) + 1
    clusters_2[clusters_2!=-1] = clusters_2[clusters_2!=-1] + ovl

    remove_clusters = [[]]
    keep_clusters = []
    og_clusters = [np.copy(clusters_1), np.copy(clusters_2)]
    
    # loop untill done
    while True:

        # compute unique clusters and cluster sizes
        # of cluster that have not been iterated over:
        c1_labels, c1_size = unique_counts(clusters_1[(clusters_1 != -1) &amp; (c1_keep == 0)])
        c2_labels, c2_size = unique_counts(clusters_2[(clusters_2 != -1) &amp; (c2_keep == 0)])

        # if all clusters are done, break from loop:
        if len(c1_size) == 0 and len(c2_size) == 0:
            break

        # if the biggest cluster is in c_p, keep this one and discard all clusters
        # on the same indices in c_t:
        elif np.argmax([np.max(np.append(c1_size, 0)), np.max(np.append(c2_size, 0))]) == 0:
            
            # remove all the mappings from the other indices
            cluster_mappings, _ = unique_counts(clusters_2[clusters_1 == c1_labels[np.argmax(c1_size)]])
            
            clusters_2[np.isin(clusters_2, cluster_mappings)] = -1
            
            c1_keep[clusters_1==c1_labels[np.argmax(c1_size)]] = 1

            remove_clusters.append(cluster_mappings)
            keep_clusters.append(c1_labels[np.argmax(c1_size)])

            if verbose &gt; 0:
                print(&#39;Keep cluster %i of group 1, delete clusters %s of group 2&#39; % (c1_labels[np.argmax(c1_size)], str(cluster_mappings[cluster_mappings!=-1] - ovl)))

        # if the biggest cluster is in c_t, keep this one and discard all mappings in c_p
        elif np.argmax([np.max(np.append(c1_size, 0)), np.max(np.append(c2_size, 0))]) == 1:
            
            # remove all the mappings from the other indices
            cluster_mappings, _ = unique_counts(clusters_1[clusters_2 == c2_labels[np.argmax(c2_size)]])
            
            clusters_1[np.isin(clusters_1, cluster_mappings)] = -1

            c2_keep[clusters_2==c2_labels[np.argmax(c2_size)]] = 1

            remove_clusters.append(cluster_mappings)
            keep_clusters.append(c2_labels[np.argmax(c2_size)])

            if verbose &gt; 0:
                print(&#39;Keep cluster %i of group 2, delete clusters %s of group 1&#39; % (c2_labels[np.argmax(c2_size)] - ovl, str(cluster_mappings[cluster_mappings!=-1])))
    
    # combine results    
    clusters = (clusters_1+1)*c1_keep + (clusters_2+1)*c2_keep - 1
    x_merged = (x_1)*c1_keep + (x_2)*c2_keep

    return clusters, x_merged, np.vstack([c1_keep, c2_keep])</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.merge_gaussians"><code class="name flex">
<span>def <span class="ident">merge_gaussians</span></span>(<span>x, labels, merge_threshold=0.1)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge all clusters which have medians which are near. Only works in 1D.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>1D array</code> of <code>ints</code> or <code>floats</code></dt>
<dd>Features used for clustering.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>1D array</code> of <code>ints</code></dt>
<dd>Labels for each sample in x.</dd>
<dt><strong><code>merge_threshold</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Similarity threshold to merge clusters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>1D array</code> of <code>ints</code></dt>
<dd>Merged labels for each sample in x.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_gaussians(x, labels, merge_threshold=0.1):
    &#34;&#34;&#34; Merge all clusters which have medians which are near. Only works in 1D.

    Parameters
    ----------
    x : 1D array of ints or floats
        Features used for clustering.
    labels : 1D array of ints
        Labels for each sample in x.
    merge_threshold : float (optional)
        Similarity threshold to merge clusters.

    Returns
    -------
    labels : 1D array of ints
        Merged labels for each sample in x.
    &#34;&#34;&#34;

    # compare all the means of the gaussians. If they are too close, merge them.
    unique_labels = np.unique(labels[labels!=-1])
    x_medians = [np.median(x[labels==l]) for l in unique_labels]

    # fill a dict with the label mappings
    mapping = {}
    for label_1, x_m1 in zip(unique_labels, x_medians):
        for label_2, x_m2 in zip(unique_labels, x_medians):
            if label_1!=label_2:
                if np.abs(np.diff([x_m1, x_m2]))/np.max([x_m1, x_m2]) &lt; merge_threshold:
                    mapping[label_1] = label_2
    # apply mapping
    for map_key, map_value in mapping.items():
        labels[labels==map_key] = map_value

    return labels</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.remove_artefacts"><code class="name flex">
<span>def <span class="ident">remove_artefacts</span></span>(<span>all_snippets, clusters, samplerate, freq_low=20000, threshold=0.75, verbose=0, return_data=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Create a mask for EOD clusters that result from artefacts, based on power in low frequency spectrum.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>all_snippets</code></strong> :&ensp;<code>2D array</code></dt>
<dd>EOD snippets. Shape=(nEODs, EOD length)</dd>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>EOD cluster labels</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Samplerate of original recording data.</dd>
<dt><strong><code>freq_low</code></strong> :&ensp;<code>float</code></dt>
<dd>Frequency up to which low frequency components are summed up.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Minimum value for sum of low frequency components relative to
sum overa ll spectrl amplitudes that separates artefact from
clean pulsefish clusters.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>return_data</code></strong> :&ensp;<code>list</code> of <code>strings (optional)</code></dt>
<dd>Keys that specify data to be logged. The key that can be used to log data in this function is
'eod_deletion' (see extract_pulsefish()).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mask</code></strong> :&ensp;<code>numpy array</code> of <code>booleans</code></dt>
<dd>Set to True for every EOD which is an artefact.</dd>
<dt><strong><code>adict</code></strong> :&ensp;<code>dictionary</code></dt>
<dd>Key value pairs of logged data. Data to be logged is specified by return_data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_artefacts(all_snippets, clusters, samplerate,
                     freq_low=20000, threshold=0.75,
                     verbose=0, return_data=[]):
    &#34;&#34;&#34; Create a mask for EOD clusters that result from artefacts, based on power in low frequency spectrum.

    Parameters
    ----------
    all_snippets: 2D array
        EOD snippets. Shape=(nEODs, EOD length)
    clusters: list of ints
        EOD cluster labels
    samplerate : float
        Samplerate of original recording data.
    freq_low: float
        Frequency up to which low frequency components are summed up. 
    threshold : float (optional)
        Minimum value for sum of low frequency components relative to
        sum overa ll spectrl amplitudes that separates artefact from
        clean pulsefish clusters.
    verbose : int (optional)
        Verbosity level.
    return_data : list of strings (optional)
        Keys that specify data to be logged. The key that can be used to log data in this function is
        &#39;eod_deletion&#39; (see extract_pulsefish()).

    Returns
    -------
    mask: numpy array of booleans
        Set to True for every EOD which is an artefact.
    adict : dictionary
        Key value pairs of logged data. Data to be logged is specified by return_data.
    &#34;&#34;&#34;
    adict = {}

    mask = np.zeros(clusters.shape, dtype=bool)

    for cluster in np.unique(clusters[clusters &gt;= 0]):
        snippets = all_snippets[clusters == cluster]
        mean_eod = np.mean(snippets, axis=0)
        mean_eod = mean_eod - np.mean(mean_eod)
        mean_eod_fft = np.abs(np.fft.rfft(mean_eod))
        freqs = np.fft.rfftfreq(len(mean_eod), 1/samplerate)
        low_frequency_ratio = np.sum(mean_eod_fft[freqs&lt;freq_low])/np.sum(mean_eod_fft)
        if low_frequency_ratio &lt; threshold:  # TODO: check threshold!
            mask[clusters==cluster] = True
            
            if verbose &gt; 0:
                print(&#39;Deleting cluster %i with low frequency ratio of %.3f (min %.3f)&#39; % (cluster, low_frequency_ratio, threshold))

        if &#39;eod_deletion&#39; in return_data:
            adict[&#39;vals_%d&#39; % cluster] = [mean_eod, mean_eod_fft]
            adict[&#39;mask_%d&#39; % cluster] = [np.any(mask[clusters==cluster])]
    
    return mask, adict</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.remove_sparse_detections"><code class="name flex">
<span>def <span class="ident">remove_sparse_detections</span></span>(<span>clusters, eod_widths, samplerate, T, min_density=0.0005, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove all EOD clusters that are too sparse</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster labels.</dd>
<dt><strong><code>eod_widths</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster widths in samples.</dd>
<dt><strong><code>samplerate</code></strong> :&ensp;<code>float</code></dt>
<dd>Samplerate.</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>float</code></dt>
<dd>Lenght of recording in seconds.</dd>
<dt><strong><code>min_density</code></strong> :&ensp;<code>float (optional)</code></dt>
<dd>Minimum density for realistic EOD detections.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int (optional)</code></dt>
<dd>Verbosity level.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code> of <code>ints</code></dt>
<dd>Cluster labels, where sparse clusters have been set to -1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_sparse_detections(clusters, eod_widths, samplerate, T,
                             min_density=0.0005, verbose=0):
    &#34;&#34;&#34; Remove all EOD clusters that are too sparse

    Parameters
    ----------
    clusters : list of ints
        Cluster labels.
    eod_widths : list of ints
        Cluster widths in samples.
    samplerate : float
        Samplerate.
    T : float
        Lenght of recording in seconds.
    min_density : float (optional)
        Minimum density for realistic EOD detections.
    verbose : int (optional)
        Verbosity level.

    Returns
    -------
    clusters : list of ints
        Cluster labels, where sparse clusters have been set to -1.
    &#34;&#34;&#34;
    for c in np.unique(clusters):
        if c!=-1:

            n = len(clusters[clusters==c])
            w = np.median(eod_widths[clusters==c])/samplerate

            if n*w &lt; T*min_density:
                if verbose&gt;0:
                    print(&#39;cluster %i is too sparse&#39;%c)
                clusters[clusters==c] = -1
    return clusters</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.subtract_slope"><code class="name flex">
<span>def <span class="ident">subtract_slope</span></span>(<span>snippets, heights)</span>
</code></dt>
<dd>
<div class="desc"><p>Subtract underlying slope from all EOD snippets.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>snippets</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>All EODs in a recorded stacked as snippets.
Shape = (number of EODs, EOD width)</dd>
<dt><strong><code>heights</code></strong> :&ensp;<code>1D numpy array</code></dt>
<dd>EOD heights.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>snippets</code></strong> :&ensp;<code>2-D numpy array</code></dt>
<dd>EOD snippets with underlying slope subtracted.</dd>
<dt><strong><code>bg_ratio</code></strong> :&ensp;<code>1-D numpy array</code></dt>
<dd>EOD height/background activity height.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtract_slope(snippets, heights):
    &#34;&#34;&#34; Subtract underlying slope from all EOD snippets.

    Parameters
    ----------
    snippets: 2-D numpy array
        All EODs in a recorded stacked as snippets. 
        Shape = (number of EODs, EOD width)
    heights: 1D numpy array
        EOD heights.

    Returns
    -------
    snippets: 2-D numpy array
        EOD snippets with underlying slope subtracted.
    bg_ratio : 1-D numpy array
        EOD height/background activity height.
    &#34;&#34;&#34;

    left_y = snippets[:,0]
    right_y = snippets[:,-1]

    try:
        slopes = np.linspace(left_y, right_y, snippets.shape[1])
    except ValueError:
        delta = (right_y - left_y)/snippets.shape[1]
        slopes = np.arange(0, snippets.shape[1], dtype=snippets.dtype).reshape((-1,) + (1,) * np.ndim(delta))*delta + left_y
    
    return snippets - slopes.T, np.abs(left_y-right_y)/heights</code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.unique_counts"><code class="name flex">
<span>def <span class="ident">unique_counts</span></span>(<span>ar)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the unique elements of an array and their counts, ignoring shape.</p>
<p>The code is condensed from numpy version 1.17.0.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ar</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Input array</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>unique_vaulues</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Unique values in array ar.</dd>
<dt><strong><code>unique_counts</code></strong> :&ensp;<code>numpy array</code></dt>
<dd>Number of instances for each unique value in ar.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unique_counts(ar):
    &#34;&#34;&#34; Find the unique elements of an array and their counts, ignoring shape.

    The code is condensed from numpy version 1.17.0.
    
    Parameters
    ----------
    ar : numpy array
        Input array

    Returns
    -------
    unique_vaulues : numpy array
        Unique values in array ar.
    unique_counts : numpy array
        Number of instances for each unique value in ar.
    &#34;&#34;&#34;
    try:
        return np.unique(ar, return_counts=True)
    except TypeError:
        ar = np.asanyarray(ar).flatten()
        ar.sort()
        mask = np.empty(ar.shape, dtype=np.bool_)
        mask[:1] = True
        mask[1:] = ar[1:] != ar[:-1]
        idx = np.concatenate(np.nonzero(mask) + ([mask.size],))
        return ar[mask], np.diff(idx)  </code></pre>
</details>
</dd>
<dt id="thunderfish.pulses.warn"><code class="name flex">
<span>def <span class="ident">warn</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Ignore all warnings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warn(*args, **kwargs):
    &#34;&#34;&#34;
    Ignore all warnings.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#main-function">Main function</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="thunderfish" href="index.html">thunderfish</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="thunderfish.pulses.BGM" href="#thunderfish.pulses.BGM">BGM</a></code></li>
<li><code><a title="thunderfish.pulses.assign_side_peaks" href="#thunderfish.pulses.assign_side_peaks">assign_side_peaks</a></code></li>
<li><code><a title="thunderfish.pulses.cluster" href="#thunderfish.pulses.cluster">cluster</a></code></li>
<li><code><a title="thunderfish.pulses.cluster_on_shape" href="#thunderfish.pulses.cluster_on_shape">cluster_on_shape</a></code></li>
<li><code><a title="thunderfish.pulses.delete_moving_fish" href="#thunderfish.pulses.delete_moving_fish">delete_moving_fish</a></code></li>
<li><code><a title="thunderfish.pulses.delete_unreliable_fish" href="#thunderfish.pulses.delete_unreliable_fish">delete_unreliable_fish</a></code></li>
<li><code><a title="thunderfish.pulses.delete_wavefish_and_sidepeaks" href="#thunderfish.pulses.delete_wavefish_and_sidepeaks">delete_wavefish_and_sidepeaks</a></code></li>
<li><code><a title="thunderfish.pulses.detect_pulse_candidates" href="#thunderfish.pulses.detect_pulse_candidates">detect_pulse_candidates</a></code></li>
<li><code><a title="thunderfish.pulses.discard_connecting_eods" href="#thunderfish.pulses.discard_connecting_eods">discard_connecting_eods</a></code></li>
<li><code><a title="thunderfish.pulses.extract_means" href="#thunderfish.pulses.extract_means">extract_means</a></code></li>
<li><code><a title="thunderfish.pulses.extract_pulsefish" href="#thunderfish.pulses.extract_pulsefish">extract_pulsefish</a></code></li>
<li><code><a title="thunderfish.pulses.extract_snippet_features" href="#thunderfish.pulses.extract_snippet_features">extract_snippet_features</a></code></li>
<li><code><a title="thunderfish.pulses.find_clipped_clusters" href="#thunderfish.pulses.find_clipped_clusters">find_clipped_clusters</a></code></li>
<li><code><a title="thunderfish.pulses.jit" href="#thunderfish.pulses.jit">jit</a></code></li>
<li><code><a title="thunderfish.pulses.merge_clusters" href="#thunderfish.pulses.merge_clusters">merge_clusters</a></code></li>
<li><code><a title="thunderfish.pulses.merge_gaussians" href="#thunderfish.pulses.merge_gaussians">merge_gaussians</a></code></li>
<li><code><a title="thunderfish.pulses.remove_artefacts" href="#thunderfish.pulses.remove_artefacts">remove_artefacts</a></code></li>
<li><code><a title="thunderfish.pulses.remove_sparse_detections" href="#thunderfish.pulses.remove_sparse_detections">remove_sparse_detections</a></code></li>
<li><code><a title="thunderfish.pulses.subtract_slope" href="#thunderfish.pulses.subtract_slope">subtract_slope</a></code></li>
<li><code><a title="thunderfish.pulses.unique_counts" href="#thunderfish.pulses.unique_counts">unique_counts</a></code></li>
<li><code><a title="thunderfish.pulses.warn" href="#thunderfish.pulses.warn">warn</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>